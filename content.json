{"meta":{"title":"技术人生","subtitle":null,"description":null,"author":"Fly","url":"https://flyingglass.github.io"},"pages":[{"title":"About","date":"2021-04-29T09:45:31.286Z","updated":"2021-04-29T09:45:31.286Z","comments":true,"path":"about/index.html","permalink":"https://flyingglass.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2021-04-29T09:45:31.287Z","updated":"2021-04-29T09:45:31.287Z","comments":true,"path":"categories/index.html","permalink":"https://flyingglass.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"mesh下Gateway落地","slug":"mesh下Gateway落地","date":"2023-03-28T07:04:17.000Z","updated":"2023-03-29T03:11:49.182Z","comments":true,"path":"2023/03/28/mesh下Gateway落地/","link":"","permalink":"https://flyingglass.github.io/2023/03/28/mesh下Gateway落地/","excerpt":"","text":"本文主要讨论Istio Ingress Gateway入口方案，网关选型为Istio Ingress Gateway（Envoy网关的一种），部署方式为中心化网关进行部署，在腾讯云落地方式目前主要考虑两种方式： CLB复用 架构图： 优点： 对现有入口调整改动小，可以复用CLB，落地周期短 可通过CLB规则控制灰度流量到URL粒度 缺点： 需维护CLB和Service NodePort的手动绑定关系，多一层的DNAT开销 长远来看随着流量的增大超过单CLB入口流量仍需采用GTM方案 iGTM 构架图： 优点： 对现有入口调整改动大，借助DNS负载均衡，实现多CLB共存，长远来看增强入口的扩展性和可用性 结合HttpDNS可实现入口流量秒级容灾，如果无需秒级可以不用开启（作为纯入口方案也可以不引入） 借助腾讯云的网关管理控制器可实现CLB与Istio Ingress Gateway的自动绑定关系，方便维护 缺点： 依赖于iGTM和Mesh的SSL证书上浮等产品的稳定性，落地周期长 HttpDNS秒级容灾会产生费用开销，费用成本上升（作为纯入口方案也可以不引入） 另附Envoy Gateway与传统的SpringCloud Gateway对比： 优点： Envoy Gateway相对于Java异步网关更能贴合K8s，与k8s的资源对象结合更紧密 Envoy Gateway在大流量、扩展性、可配置性方面强于Java异步网关 Envoy Gateway是面向未来的网关选型，相信社区会逐步增强和标准化Envoy网关产品以适应更广泛的应用场景 缺点 Envoy Gateway在领域认知度和技术栈熟悉方面方面逊于Java异步网关 基于Envoy Gateway定制化扩展的框架较少，定制化难度目前相对较高 参考链接：亲历者复盘：网易的 Envoy 网关选型、开发与改造 诗和远方：蚂蚁金服 Service Mesh 深度实践 | QCon 实录","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Mesh","slug":"架构之路/Mesh","permalink":"https://flyingglass.github.io/categories/架构之路/Mesh/"}],"tags":[]},{"title":"mesh腾讯云物理架构","slug":"mesh腾讯云物理架构","date":"2023-03-23T08:19:21.000Z","updated":"2023-03-29T03:11:44.268Z","comments":true,"path":"2023/03/23/mesh腾讯云物理架构/","link":"","permalink":"https://flyingglass.github.io/2023/03/23/mesh腾讯云物理架构/","excerpt":"","text":"","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Mesh","slug":"架构之路/Mesh","permalink":"https://flyingglass.github.io/categories/架构之路/Mesh/"}],"tags":[]},{"title":"mesh与cvm的对比","slug":"mesh与cvm的对比","date":"2023-03-22T06:52:41.000Z","updated":"2023-03-29T03:11:54.391Z","comments":true,"path":"2023/03/22/mesh与cvm的对比/","link":"","permalink":"https://flyingglass.github.io/2023/03/22/mesh与cvm的对比/","excerpt":"","text":"Mesh与CVM的对比物理对比 物理架构 mesh腾讯云物理架构.md 产品分布 CVM：云服务器 Mesh：容器服务（TKE），只读权限 监控对比 日志 业务日志 CVM: ES，通过host.hostname区分 Mesh：ES，通过pod.namespace，pod.ip，pod.name区分 Tomcat访问日志 CVM：cvm机器，通过jumpserver访问cvm机器 Mesh：ES，通过pod.namespace，pod.ip，pod.name区分 GC日志 CVM：同Tomcat访问日志 Mesh：原理是通过kubectl查询，UI可采用Zadig或者Tke控制台查看 Sentinel日志 CVM：同Tomcat访问日志 Mesh：存储介质为cfs，可通过cvm机器挂载后查看 指标 Prometheus CVM：原理是通过JVM应用获取并上传，新增cvm需运维手动绑定 Mesh：原理同cvm，新增pod会自动绑定 Cat CVM：Cat Client埋点上传 Mesh：同cvm 链路 Skywalking CVM：javaagent引入Skywalking Agent，由插件织入埋点上传 Mesh：同cvm，sidecar埋点暂缺失 排障对比 日志 报错日志 CVM：结合具体日志，通过metadata前往cvm下监控系统的日志、指标、链路定位和处理，对象主要为cvm机器 Mesh：结合具体日志，通过metadata前往Mesh下监控系统的日志、指标、链路定位和处理，对象主要为k8s 权限 权限划分 CVM：用户账号划分 Mesh：登录容器与应用进程为同一用户，看运维最终如何处理？ 工具 安装 CVM：申请运维安装，cvm安装一次后终身存在 Mesh：申请运维安装，pod安装一次后，重启即消失，常用工具可提前放置镜像 调试 CVM：进入cvm简单调试，深度调试需要申请运维进行账号权限提级，并且可以单独摘除流量查看 Mesh：通过k8s简单调试，深度调试情况比较复杂，目前借助nacos在应用层面可以单独摘除流量查看","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Mesh","slug":"架构之路/Mesh","permalink":"https://flyingglass.github.io/categories/架构之路/Mesh/"}],"tags":[]},{"title":"mesh服务改造手册","slug":"mesh服务改造手册","date":"2023-03-22T06:40:58.000Z","updated":"2023-03-29T03:11:25.618Z","comments":true,"path":"2023/03/22/mesh服务改造手册/","link":"","permalink":"https://flyingglass.github.io/2023/03/22/mesh服务改造手册/","excerpt":"","text":"Mesh改造镜像 容器相关：用于构建镜像，运行容器 Dockerfile 1234567891011FROM mdd-images.tencentcloudcr.com/mdd/alijdk:8.8.9_centosARG PROJ_DIR=/data/tvbcserver/mddWORKDIR $PROJ_DIRCOPY mdd-community-api.jar $PROJ_DIRCOPY start.sh $PROJ_DIRCOPY preStop.sh $PROJ_DIRRUN chmod +x start.sh &amp;&amp; chmod +x preStop.shCMD [\"/bin/bash\", \"-c\", \"/data/tvbcserver/mdd/start.sh\"] start.sh 123456789101112131415161718192021222324252627#!/usr/bin/env bash# logs[[ -d logs ]] || mkdir logs# [[ -d logs/app ]] || mkdir -p logs/app# [[ -d logs/cfs ]] || mkdir -p logs/cfs# chmod -R 777 logs/mdd-community# JAVA_OPTSif [ -z \"$JAVA_OPTS\" ];then JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -Xms1g -Xmx1g -Xmn512m -XX:MetaspaceSize=256m -Xss256k\" JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSScavengeBeforeRemark -XX:+ParallelRefProcEnabled\" JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:logs/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=logs/oom-error.hprof\" JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -Djava.net.preferIPv4Stack=true -Duser.timezone=Asia/Shanghai -Dfile.encoding=UTF-8 -DJM.LOG.PATH=logs -DJM.SNAPSHOT.PATH=/data/tvbcserver\"# JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -javaagent:/data/tvbcserver/mdd/mddagent/mdd-agent-starter.jar -Dlily.agent.servlet.enabled=true\"# JAVA_OPTS=\"$&#123;JAVA_OPTS&#125; -javaagent:/data/tvbcserver/skywalking/agent/skywalking-agent.jar=collector.backend_service=172.16.80.4:11800,agent.service_name=mdd::mdd-community\"else JAVA_OPTS=\"$JAVA_OPTS\"fi# JAVA_ARGSif [ -z \"$JAVA_ARGS\" ];then JAVA_ARGS=\"--spring.cloud.nacos.discovery.metadata.env=dev --feign.mdd.load-balancer.enabled=false\"else JAVA_ARGS=\"$JAVA_ARGS\"fijava $JAVA_OPTS -jar mdd-community-api.jar $JAVA_ARGS preStop.sh：用于Lily规则修改，用于打通Nacos的CVM集群 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/bin/sh# console地址consoleAddress=$CONSOLE_ADDRESS# 应用名称appName=$APP_NAME# 实例地址address=$&#123;POD_IP&#125;:$&#123;POD_PORT&#125;# Nacos 命名空间IDnacosNamespaceId=$NACOS_NAMESPACE# Nacos GroupnacosGroup=$NACOS_GROUP# 随机数rand() &#123; min=$1 max=$(($2-$min+1)) num=$(($RANDOM+1000000000)) #增加一个10位的数再求余 echo $(($num%$max+$min))&#125;# 加入黑名单removeInstance() &#123; result=$(curl --location --request POST 'http://'$&#123;consoleAddress&#125;'/lily/console/release/lb/remove-instance' --header 'Content-Type: application/json' --data '&#123; \"namespaceId\":\"'$&#123;nacosNamespaceId&#125;'\", \"group\":\"'$&#123;nacosGroup&#125;'\", \"address\":\"'$&#123;address&#125;'\", \"serviceId\":\"'$&#123;appName&#125;'\" &#125;') if [ -z \"$result\" ];then echo '第'$1'次调用接口失败' else echo '第'$1'次调用接口结果：'$result fi&#125;# 累计重试次数count=1# 间隔0-500mswhile true;do if [ $count != 1 ]; then sleepTime=$(echo \"scale=3; $(rand 0 500)/1000\" | bc) sleep $sleepTime fi # 调用函加入黑名单 removeInstance $count # 判断是否成功 if [ ! -z \"$result\" ]&amp;&amp;[ $result == true ]; then break fi # 累计重试次数 count=$(($count+1))done Makefile：用于zadig故障的备用构建 业务相关：日志改造和Sentinel排除掉探针 SentinelUrlcleaner：探针屏蔽 123456789101112131415161718192021222324@Componentpublic class MddSentinelUrlCleaner implements UrlCleaner &#123; private static final PathMatcher PATH_MATCHER = new AntPathMatcher(); private static final List&lt;String&gt; IGNORE_URLS = Arrays.asList( \"/\",\"/error\",\"/actuator/**\" ); /*** * &lt;p&gt;Process the url. Some path variables should be handled and unified.&lt;/p&gt; * &lt;p&gt;e.g. collect_item_relation--10200012121-.html will be converted to collect_item_relation.html&lt;/p&gt; * * @param originUrl original url * @return processed url */ @Override public String clean(String originUrl) &#123; for (String s : IGNORE_URLS) &#123; if (PATH_MATCHER.match(s, originUrl)) &#123; return null; &#125; &#125; return originUrl; &#125;&#125; logback-spring.xml：日志改造 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180&lt;!-- Logback configuration. See http://logback.qos.ch/manual/index.html --&gt;&lt;configuration scan=\"true\" scanPeriod=\"10 seconds\" debug=\"false\"&gt;&lt;!-- k8s专用，主要优先兼容cvm日志，当同时存在cvm和k8s时，无须处理cvm日志清洗和兼容问题--&gt;&lt;!-- so, add additional logback-spring.xml, which can make life easy.--&gt; &lt;springProperty scope=\"context\" name=\"contextName\" source=\"spring.application.name\" defaultValue=\"mdd-community\"/&gt; &lt;springProperty scope=\"context\" name=\"appDir\" source=\"logback.appDir\" defaultValue=\"logs/app\"/&gt; &lt;springProperty scope=\"context\" name=\"cfsDir\" source=\"logback.cfsDir\" defaultValue=\"logs/cfs\"/&gt; &lt;springProperty scope=\"context\" name=\"cspDir\" source=\"logback.cspDir\" defaultValue=\"logs/csp\"/&gt; &lt;contextName&gt;$&#123;contextName&#125;&lt;/contextName&gt; &lt;property name=\"FILE_PATTERN\" value=\"%d&#123;HH:mm:ss.SSS&#125; $&#123;CONTEXT_NAME&#125; [%thread] %-5level %logger&#123;5&#125; [%line] | [%X&#123;trace-id&#125;] [%X&#123;span-id&#125;] [%X&#123;l-h-s-group&#125;] [%X&#123;l-h-s-type&#125;] [%X&#123;l-h-s-id&#125;] [%X&#123;l-h-s-address&#125;] [%X&#123;l-h-s-version&#125;] [%X&#123;l-h-s-region&#125;] [%X&#123;l-h-s-env&#125;] [%X&#123;l-h-s-zone&#125;] | %msg%n\"/&gt; &lt;appender name=\"infoAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;appDir&#125;/info.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;appDir&#125;/info-%d&#123;yyyy-MM-dd&#125;_%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;5GB&lt;/maxFileSize&gt; &lt;maxHistory&gt;3&lt;/maxHistory&gt; &lt;totalSizeCap&gt;15GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;FILE_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name=\"warnAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;appDir&#125;/warn.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;appDir&#125;/warn-%d&#123;yyyy-MM-dd&#125;_%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;1GB&lt;/maxFileSize&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;totalSizeCap&gt;5GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;FILE_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;WARN&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name=\"errorAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;appDir&#125;/error.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;appDir&#125;/error-%d&#123;yyyy-MM-dd&#125;_%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;1GB&lt;/maxFileSize&gt; &lt;maxHistory&gt;10&lt;/maxHistory&gt; &lt;totalSizeCap&gt;10GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;FILE_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class=\"ch.qos.logback.classic.filter.LevelFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"infoAppender\"/&gt; &lt;appender-ref ref=\"warnAppender\"/&gt; &lt;appender-ref ref=\"errorAppender\"/&gt; &lt;/root&gt; &lt;!-- ============================ Vod Begin ============================ --&gt; &lt;!-- 点播分集接口请求日志--&gt; &lt;appender name=\"VodAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;cfsDir&#125;/vod-%d&#123;yyyy-MM-dd.HH&#125;_%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;500MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;72&lt;/maxHistory&gt; &lt;totalSizeCap&gt;5GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 点播分集接口响应日志--&gt; &lt;appender name=\"SactionAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;cfsDir&#125;/saction-%d&#123;yyyy-MM-dd.HH&#125;_%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;500MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;72&lt;/maxHistory&gt; &lt;totalSizeCap&gt;5GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 防盗链异步写入cfs--&gt; &lt;appender name=\"asyncVodAppender\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt; &lt;appender-ref ref=\"VodAppender\"/&gt; &lt;neverBlock&gt;true&lt;/neverBlock&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;queueSize&gt;1024&lt;/queueSize&gt; &lt;/appender&gt; &lt;!-- 防盗链异步写入cfs--&gt; &lt;appender name=\"asyncSactionAppender\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt; &lt;appender-ref ref=\"VodAppender\"/&gt; &lt;neverBlock&gt;true&lt;/neverBlock&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;queueSize&gt;1024&lt;/queueSize&gt; &lt;/appender&gt; &lt;logger name=\"com.pureShare.common.log.VodLog\" level=\"INFO\" additivity=\"false\"&gt; &lt;appender-ref ref=\"asyncVodAppender\"/&gt; &lt;/logger&gt; &lt;logger name=\"com.pureShare.common.log.SactionLog\" level=\"INFO\" additivity=\"false\"&gt; &lt;appender-ref ref=\"asyncSactionAppender\"/&gt; &lt;/logger&gt; &lt;!-- ============================ Vod End ============================ --&gt; &lt;!-- ============================ Sentinel Begin ============================ --&gt; &lt;appender name=\"sentinelRecordAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;cspDir&#125;/sentinel-record.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;cspDir&#125;/sentinel-record.log.%d&#123;yyyy-MM-dd&#125;_%i&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"sentinelCommandCenterAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;file&gt;$&#123;cspDir&#125;/sentinel-command-center.log&lt;/file&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"&gt; &lt;fileNamePattern&gt;$&#123;cspDir&#125;/sentinel-command-center.log.%d&#123;yyyy-MM-dd&#125;_%i&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- Sentinel异步共用--&gt; &lt;appender name=\"asyncSentinelRecordAppender\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt; &lt;appender-ref ref=\"sentinelRecordAppender\"/&gt; &lt;neverBlock&gt;true&lt;/neverBlock&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;/appender&gt; &lt;appender name=\"asyncSentinelCommandCenterAppender\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt; &lt;appender-ref ref=\"sentinelCommandCenterAppender\"/&gt; &lt;neverBlock&gt;true&lt;/neverBlock&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;/appender&gt; &lt;!-- 参考Sentinel RecordLogLogger--&gt; &lt;logger name=\"sentinelRecordLogger\" level=\"INFO\" additivity=\"false\"&gt; &lt;appender-ref ref=\"asyncSentinelRecordAppender\"/&gt; &lt;/logger&gt; &lt;!-- 参考Sentinel CommandCenterLogLogger--&gt; &lt;logger name=\"sentinelCommandCenterLogger\" level=\"INFO\" additivity=\"false\"&gt; &lt;appender-ref ref=\"asyncSentinelCommandCenterAppender\"/&gt; &lt;/logger&gt; &lt;!-- ============================ Sentinel End ============================ --&gt;&lt;/configuration&gt; POD Deployment描述文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161apiVersion: apps/v1kind: Deploymentmetadata: name: mdd-community-v1 namespace: prod labels: app: mdd-community version: v1spec: replicas: 4 selector: matchLabels: app: mdd-community version: v1 minReadySeconds: 11 template: metadata: labels: business: mdd app: mdd-community version: v1 annotations: # 兼容cvm,绕行端口,迁移完毕,采用subnet traffic.sidecar.istio.io/excludeOutboundPorts: 3306,6379,63790,5035,11800,2280 eks.tke.cloud.tencent.com/cpu: '4' eks.tke.cloud.tencent.com/mem: '8Gi' eks.tke.cloud.tencent.com/root-cbs-size: '50' spec: nodeSelector: node.kubernetes.io/instance-type: eklet initContainers: - name: init-skywalking-agent image: mdd-images.tencentcloudcr.com/mdd/agents:latest command: ['sh', '-c', 'cp -rf /tmp/skywalking /data/tvbcserver/agent/;cp -rf /tmp/mddagent /data/tvbcserver/agent/;'] volumeMounts: - name: ed-agent mountPath: /data/tvbcserver/agent containers: - name: mdd-community image: mdd-images.tencentcloudcr.com/prod/mdd-community imagePullPolicy: Always ports: - containerPort: 8101 env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: APP_NAME valueFrom: fieldRef: fieldPath: metadata.labels['app'] - name: NACOS_NAMESPACE value: \"\" - name: NACOS_GROUP value: \"mdd\" - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: POD_PORT value: \"8101\" - name: CONSOLE_ADDRESS value: \"prod.istio.mddcloud.com.cn\" - name: JAVA_OPTS value: &gt;- -XX:ActiveProcessorCount=4 -Xms4g -Xmx4g -Xmn2g -XX:MetaspaceSize=256m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSScavengeBeforeRemark -XX:+ParallelRefProcEnabled -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:logs/gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=logs/oom-error.hprof -Djava.net.preferIPv4Stack=true -Duser.timezone=Asia/Shanghai -Dfile.encoding=UTF-8 -DJM.LOG.PATH=logs -DJM.SNAPSHOT.PATH=/data/tvbcserver -javaagent:/data/tvbcserver/agent/mddagent/mdd-agent-starter.jar -Dlily.agent.servlet.enabled=true -javaagent:/data/tvbcserver/agent/skywalking/agent/skywalking-agent.jar=collector.backend_service=172.16.80.4:11800,agent.service_name=mdd::mdd-community-api,agent.sample_n_per_3_secs=1 - name: JAVA_ARGS value: &gt;- --spring.cloud.nacos.discovery.metadata.env=prod --feign.mdd.load-balancer.enabled=false --server.tomcat.accesslog.max-days=3 --logging.config=classpath:k8s/logback-spring.xml lifecycle: preStop: exec: command: - /bin/sh - '-c' - /data/tvbcserver/mdd/preStop.sh;exit 0 startupProbe: httpGet: path: /actuator/health port: 8101 httpHeaders: - name: Authorization value: Basic bW9uaXRvcjp0dmJjMTIz initialDelaySeconds: 50 periodSeconds: 5 failureThreshold: 14 readinessProbe: httpGet: path: /actuator/health/readiness port: 8101 httpHeaders: - name: Authorization value: Basic bW9uaXRvcjp0dmJjMTIz periodSeconds: 5 volumeMounts: - name: ed-agent mountPath: /data/tvbcserver/agent - name: ed-logs mountPath: /data/tvbcserver/mdd/logs - name: cfs-logs mountPath: /data/tvbcserver/mdd/logs/cfs subPathExpr: $(POD_NAMESPACE)/$(APP_NAME)/$(POD_NAME)/cfs - name: cfs-logs mountPath: /data/tvbcserver/mdd/logs/csp subPathExpr: $(POD_NAMESPACE)/$(APP_NAME)/$(POD_NAME)/csp - name: filebeat image: mdd-images.tencentcloudcr.com/mdd/mdd-filebeat:latest imagePullPolicy: Always securityContext: runAsUser: 0 env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - name: cm-filebeat mountPath: /opt/filebeat_logs/filebeat.yml subPath: filebeat.yml readOnly: true - name: ed-logs mountPath: /data/tvbcserver/mdd/logs volumes: - name: cm-filebeat configMap: name: cm-filebeat-community - name: ed-agent emptyDir: &#123;&#125; - name: ed-logs emptyDir: &#123;&#125; - name: cfs-logs persistentVolumeClaim: claimName: pvc-cfs Pod依赖文件： ConfigMap：filebeat.yml PV/PVC：cfs引用pv/pvc EnvoyFilter：Sidecar基座修改Tcp Istio IngressGateway：gateway和virtualservice修改 Nacos2Istio：迁移期间，新增服务需配置同步任务 Zadig 构建：脚本、缓存、构建容器 1234567891011121314151617181920#!/bin/bashset -ePROJ_DIR=$WORKSPACE/Mdd_AppApi# Mavencd $PROJ_DIRmvn clean package -Pa-community,prod -DskipTests# DockerTARGET=$PROJ_DIR/targetDOCKER=$PROJ_DIR/docker/communityDOCKER_TMP=$TARGET/docker_tmp[[ -d $DOCKER_TMP ]] || mkdir -p $DOCKER_TMPcp $DOCKER/Dockerfile $DOCKER_TMPcp $DOCKER/preStop.sh $DOCKER_TMPcp $DOCKER/start.sh $DOCKER_TMPcp $TARGET/mdd-community-api.jar $DOCKER_TMP# cp $TARGET/mdd-community.jar $DOCKER_TMP &amp;&amp; cd $DOCKER_TMP &amp;&amp; jar -xf mdd-community.jar 发布 建立zadig服务，以Deployment为核心，位于mdd-k8s-deploy代码仓库 建立zadig环境，关联namespace和tcr镜像仓库 建立zadig工作流，关联对应的服务和环境","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Mesh","slug":"架构之路/Mesh","permalink":"https://flyingglass.github.io/categories/架构之路/Mesh/"}],"tags":[]},{"title":"jmx远程调试","slug":"jmx远程调试","date":"2021-09-14T08:44:22.000Z","updated":"2021-09-14T08:49:47.384Z","comments":true,"path":"2021/09/14/jmx远程调试/","link":"","permalink":"https://flyingglass.github.io/2021/09/14/jmx远程调试/","excerpt":"","text":"Remote Debug开启 开启远程调试 1-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8739 idea开启debug","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"lily新手入门","slug":"lily新手入门","date":"2021-08-30T07:10:06.000Z","updated":"2023-03-29T03:09:25.181Z","comments":true,"path":"2021/08/30/lily新手入门/","link":"","permalink":"https://flyingglass.github.io/2021/08/30/lily新手入门/","excerpt":"","text":"概述元数据定义123456789101112131415spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 metadata: group: mdd version: 1.0 region: region1 env: env1 zone: zone1 config: server-addr: 127.0.0.1:8848 file-extension: yaml name: $&#123;spring.application.name&#125; Pom引入 Mdd Dependencies 123456789101112131415161718192021222324252627&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.mdd&lt;/groupId&gt; &lt;artifactId&gt;mdd-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;version.mdd-dependencies&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;version.spring-cloud&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;version.spring-cloud-alibaba&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; SpringCloud Gateway 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.mdd&lt;/groupId&gt; &lt;artifactId&gt;mdd-cloud-gateway-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mdd&lt;/groupId&gt; &lt;artifactId&gt;mdd-lily-strategy-mdc-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- &lt;dependency&gt;--&gt; &lt;!-- &lt;groupId&gt;com.mdd&lt;/groupId&gt;--&gt; &lt;!-- &lt;artifactId&gt;mdd-lily-strategy-skywalking-starter&lt;/artifactId&gt;--&gt; &lt;!-- &lt;/dependency&gt;--&gt;&lt;/dependencies&gt; Service 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.mdd&lt;/groupId&gt; &lt;artifactId&gt;mdd-cloud-servlet-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mdd&lt;/groupId&gt; &lt;artifactId&gt;mdd-lily-strategy-mdc-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- &lt;dependency&gt;--&gt; &lt;!-- &lt;groupId&gt;com.mdd&lt;/groupId&gt;--&gt; &lt;!-- &lt;artifactId&gt;mdd-lily-strategy-skywalking-starter&lt;/artifactId&gt;--&gt; &lt;!-- &lt;/dependency&gt;--&gt;&lt;/dependencies&gt; Http Header推送 配置中心推送","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"微服务","slug":"架构之路/微服务","permalink":"https://flyingglass.github.io/categories/架构之路/微服务/"}],"tags":[]},{"title":"lily解决方案","slug":"lily解决方案","date":"2021-08-30T06:35:13.000Z","updated":"2023-03-29T03:10:00.938Z","comments":true,"path":"2021/08/30/lily解决方案/","link":"","permalink":"https://flyingglass.github.io/2021/08/30/lily解决方案/","excerpt":"","text":"概述核心模块主要分为注册中心、配置中心、管理中心、策略编排 注册中心 基于Nacos注册中心，接管ServiceRegistry和ServerList处理Instance的Metadata(version，env，group，region，zone) 配置中心 基于Nacos配置中心，处理本地、远程等配置更新 管理中心 暴露Rest Endpoint接口，提供核心Api 策略编排 实现网关和服务的路由功能 处理Header参数的拦截、传递 处理策略的过滤匹配 输出调用链和日志 全链路监控 调用链监控 基于SkyWalking链路 输出l-h-*链路策略信息 输出l-h-s-*输出服务自身信息 日志监控 基于MDC链路 输出l-h-*链路策略信息 输出l-h-s-*输出服务自身信息 元数据自定义 本地配置 本地Metadata(version，env，group，region，zone)配置 header配置 http请求header指定Metadata 注册中心配置 注册中心动态化，依赖Nacos","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"微服务","slug":"架构之路/微服务","permalink":"https://flyingglass.github.io/categories/架构之路/微服务/"}],"tags":[]},{"title":"lily概述","slug":"lily概述","date":"2021-08-30T05:58:54.000Z","updated":"2023-03-29T03:09:46.904Z","comments":true,"path":"2021/08/30/lily概述/","link":"","permalink":"https://flyingglass.github.io/2021/08/30/lily概述/","excerpt":"","text":"概述核心功能基于Spring Cloud、Spring Cloud Alibaba服务注册发现，Ribbon负载均衡，Feign和Rest Template调用，Spring Cloud Gateway 全链路版本、环境、IP地址和端口匹配动态路由 全链路自定义网关、服务的过滤器、负载均衡策略发布 全链路条件匹配、非条件匹配 服务实时性流量无损下线：IP和端口屏蔽 异步场景下全链路追踪和路由：异步跨线程Agent插件 全链路调用链追踪、日志监测 提供服务端、消费端隔离、注册级别隔离和准入 本地和远程，局部和全局配置策略驱动 配置中心（Nacos）、Swagger和Rest规则策略推送 基于Header参数化规则策略驱动 限流、熔断、降级防护 版本兼容列表 Mdd Dependencies Version Spring Cloud Version Spring Cloud Alibaba Version Spring Boot Version 2.0.0 Hoxton.SR9 2.2.5.RELEASE 2.3.8.RELEASE 概念介绍 滚动发布 概念：每次滚动升级一个或者多个服务，升级完成后监控观察，直到所有旧版本服务升级到新版本。属于有损发布 优点：升级快捷，影响范围小，只会影响滚动发布的服务。 缺点：在滚动升级过程中，无法快速无损回滚，必须降级部署 全链路路由","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"微服务","slug":"架构之路/微服务","permalink":"https://flyingglass.github.io/categories/架构之路/微服务/"}],"tags":[]},{"title":"Jedis引发GC问题","slug":"Jedis引发GC问题","date":"2021-05-20T02:20:48.000Z","updated":"2023-03-29T03:20:28.536Z","comments":true,"path":"2021/05/20/Jedis引发GC问题/","link":"","permalink":"https://flyingglass.github.io/2021/05/20/Jedis引发GC问题/","excerpt":"","text":"背景介绍线上应用经常收到跨服务间SocketTimeout的异常，服务间的调用设置超时时间为500ms，查看Cat监控发现，发生超时异常都伴随着FGC，并且FGC时间都大于500ms，于是决定dump heap镜像。 问题定位分析通过mat查看heap dump镜像，发现Finalizer引用队列在活跃对象占比高达31.43%，猜测可能有泄漏。 进一步分析发现，SocksSocketImpl对象高达113529多个，大量的对象都是Jedis链接，由此确定Jedis可能发生泄漏，同时查看Old区空间稳步斜率增长，怀疑对象未充分YGC即进入Old区，造成下次FGC较长的STW，引发超时。 GC问题验证接下来就是验证上述猜测了，通过获取gc相关的参数，查看JVM各区的容量，发现S0和S1容量被调整非常小大约2m左右，结合收集器配置，情况如下： 应用采用默认的PS和PO收集器，并且默认开启-XX:+UseAdaptiveSizePolicy，系统会通过吞吐量（cpu的gc时间）计算，优先保证吞吐量来分配Eden，From，To的size，最终导致S0,S1区降为2m。结合GC日志和Cat监控，导致Old的问题就明朗了，每次YGC结束，由于PS和PO收集器的缘故，S0和S1区太小无法，导致配置的晋升参数8几乎失效，对象晋升过快被拷贝到Old区，于是添加-XX:+PrintTenuringDistribution上线验证，得到如下大量GC日志： Desired survivor size 5048576 bytes, new threshold 1 (max 8) GC问题调整上述日志验证猜想，对象由于S0和S1的size太小，晋升过快，问题得到验证，解决就简单了： 更换收集器，PN+CMS或者G1均可 仍然采用PS+PO收集器，关闭-XX:-UseAdaptiveSizePolicy，手动设置Xmn和SurvivorRatio的值 考虑heap内存比较小，4g左右，根据R大的一些分享，采用方案2，进行解决，通过Cat获取S0，S1，Eden的大小以及After FGC后活跃对象的占比，设置对应参数即可，以下来自于美团的设置分享： 根据项目实际监控情况，我们未完全上述经验值，对项目进行更合适的设置如下： 12345-Xms4G -Xmx4G -Xmn2560M -XX:MetaspaceSize=256M-XX:-UseAdaptiveSizePolicy-XX:MaxTenuringThreshold=15 -XX:TargetSurvivorRatio=80 -XX:SurvivorRatio=18 Jedis问题解决上述Jedis垃圾一方面由于GC配置参数不合理导致晋升不合理，查看Jedis的连接池的配置，也进行了调整，查看GenericObjectPool代码，定时任务检测驱逐对象，关键代码如下： 从上面代码我们看出，每隔一段时间，就会检测对象池里面对象，要是发现对象空闲时间超过一定时间，就会强制回收；然后又发现链接少于minIdle了，开始创建对象，以满足mindle。调整Redis client 设置的检测轮询时间为1分钟，设置miniIdle为5，修改后上线观察。 调整前后对比：调整前： 调整后： 通过上述对比可以看到调整后YGC的次数比调整前减半，调整后的Oldgen也几乎稳定不增长了，YGC由于少了大量的From区往Old区的拷贝（减少Survivor拷贝，GC标记较快，主要耗时为拷贝），YGC时间耗时大幅下降，GC吞吐量获得提升。 参考链接:JVM GC 之「AdaptiveSizePolicy」实战","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"问题集锦","slug":"架构之路/问题集锦","permalink":"https://flyingglass.github.io/categories/架构之路/问题集锦/"}],"tags":[]},{"title":"Kafka选举导致的问题","slug":"Kafka选举导致的问题","date":"2021-05-06T11:16:21.000Z","updated":"2021-05-06T10:24:30.008Z","comments":true,"path":"2021/05/06/Kafka选举导致的问题/","link":"","permalink":"https://flyingglass.github.io/2021/05/06/Kafka选举导致的问题/","excerpt":"","text":"线上Kafka集群由于分区达到上限，出现了选举，ISR列表发生了变更，通过kibana的日志分析： 单个异常栈如下： 消费业务代码如下： 最终Future的超时代码如下： 根据堆栈和日志追踪到这里： kafka阻塞目前会存在两个地方： 首次拉取Metadata会阻塞 由于Kafka的模型，Sender线程会处理RecordAccumulator，收集器的默认大小32M 修改方案： acks=1和retries=0参数矛盾，需查看retries代码是否会阻塞，印象中是扔到收集器后返回。 业务调用代码改成异步线程，send方法采用回调函数机制（应对第一次拉取Metadata卡住） 收集器容量塞满，目前是默认60s阻塞","categories":[{"name":"大数据","slug":"大数据","permalink":"https://flyingglass.github.io/categories/大数据/"}],"tags":[]},{"title":"数据库虚引用堆积问题","slug":"数据库虚引用堆积问题","date":"2021-05-06T07:52:42.000Z","updated":"2023-03-29T03:20:46.026Z","comments":true,"path":"2021/05/06/数据库虚引用堆积问题/","link":"","permalink":"https://flyingglass.github.io/2021/05/06/数据库虚引用堆积问题/","excerpt":"","text":"线上出现FGC占用时间过长的告警，查看GC日志确认FGC达到3.84s 问题描述 通过mat分析dump下来的日志发现，com.mysql.cj.jdbc.AbandonedConnectionCleanupThread 对象占了堆内存的大部分空间. 查看对象是com.mysql.cj.jdbc.AbandonedConnectionCleanupThread$ConnectionFinalizerPhantomReference（mysql的connection清理的虚引用）对象堆积达到2435个，初步判断长时间gc的问题是由于ConnectionFinalizerPhantomReference堆积引起的，该应用采用hikari的datasource. 同样选了一台采用druid的datasource的应用，也发现同样问题： 其中com.mysql.cj.jdbc.NonRegisteringDriver堆积对象达到1188个 2. 问题分析基本可以确定是mysql driver连接弱引用的问题，两个问题可以合并为同一个问题，其中mysql清理源代码这里略过不表，有兴趣可以去查看NonRegisteringDriver中connectionPhantomRefs的添加和清理逻辑。 这里结合项目中hikaricp数据配置和官方文档结合说明，Druid同理。 查阅hikaricp数据池的官网地址，看看部分属性介绍如下： maximumPoolSize This property controls the maximum size that the pool is allowed to reach, including both idle and in-use connections. Basically this value will determine the maximum number of actual connections to the database backend. A reasonable value for this is best determined by your execution environment. When the pool reaches this size, and no idle connections are available, calls to getConnection() will block for up to connectionTimeout milliseconds before timing out. Please read about pool sizing. Default: 10 maximumPoolSize控制最大连接数，默认为10 minimumIdle This property controls the minimum number of idle connections that HikariCP tries to maintain in the pool. If the idle connections dip below this value and total connections in the pool are less than maximumPoolSize, HikariCP will make a best effort to add additional connections quickly and efficiently. However, for maximum performance and responsiveness to spike demands, we recommend not setting this value and instead allowing HikariCP to act as a fixed size connection pool. Default: same as maximumPoolSize minimumIdle控制最小连接数，默认等同于maximumPoolSize，10。 ⌚idleTimeout This property controls the maximum amount of time that a connection is allowed to sit idle in the pool. This setting only applies when minimumIdle is defined to be less than maximumPoolSize. Idle connections will not be retired once the pool reaches minimumIdle connections. Whether a connection is retired as idle or not is subject to a maximum variation of +30 seconds, and average variation of +15 seconds. A connection will never be retired as idle before this timeout. A value of 0 means that idle connections are never removed from the pool. The minimum allowed value is 10000ms (10 seconds). Default: 600000 (10 minutes) 连接空闲时间超过idleTimeout（默认10分钟）后，连接会被抛弃 ⌚maxLifetime This property controls the maximum lifetime of a connection in the pool. An in-use connection will never be retired, only when it is closed will it then be removed. On a connection-by-connection basis, minor negative attenuation is applied to avoid mass-extinction in the pool. We strongly recommend setting this value, and it should be several seconds shorter than any database or infrastructure imposed connection time limit. A value of 0 indicates no maximum lifetime (infinite lifetime), subject of course to the idleTimeout setting. Default: 1800000 (30 minutes) 连接生存时间超过 maxLifetime（默认30分钟）后，连接会被抛弃. 回头看看项目的hikari配置： 配置了minimumIdle = 10，maximumPoolSize = 10，没有配置idleTimeout和maxLifetime。所以这两项会使用默认值 idleTimeout = 10分钟，maxLifetime = 30分钟。 假如数据库连接池已满，有10个连接，假如系统空闲(没有连接会在10分钟后（超过idleTimeout）被废弃)；假如系统一直繁忙，10个连接会在30分钟后（超过maxLifetime）后被废弃。 猜测问题产生的根源： 每次新建一个数据库连接，都会把连接放入connectionPhantomRefs集合中。数据连接在空闲时间超过idleTimeout或生存时间超过maxLifetime后会被废弃，在connectionPhantomRefs集合中等待回收。由于连接资源一般存活周期长，经过多次Young GC,一般都能存活到老年代。如果这个数据库连接对象晋升到老年代，connectionPhantomRefs中的元素就会一直堆积，直到下次 full gc。如果等到full gc 的时候connectionPhantomRefs集合的元素非常多，那么该次full gc就会非常耗时。 3. 问题验证 线上模拟环境(待测试） 为了验证问题，可以模拟线上环境，调整maxLifetime等参数~压测思路如下： 1.缓存系统模拟线上的配置，使用压测系统一段时间内持续压缓存系统，使缓存系统短时间创建/废弃大量数据库连接，观察 connectionPhantomRefs对象是否如期大量堆积，手动触发FGC，观察 connectionPhantomRefs对象是否被清理。 2.调整maxLifetime 参数，观察相同的压测时间内 connectionPhantomRefs对象是否还发生堆积 不过可以结合GC日志分析： 再结合我们生产的问题，假设我们每天14个小时高峰期(12:00 ～ 凌晨2:00)，期间连接数10，10个小时低峰期，期间连接数10，每次 full gc 间隔4天，等到下次 full gc 堆积的 NonRegisteringDriver 对象为 10 24 2 * 4 = 1920，与问题dump里面connectionPhantomRefs对象的数量2435个基本吻合。 4. 问题解决方案由上面分析可知，问题产生的根源是废弃的数据库连接对象堆积，最终导致 full gc 时间过长。可以从以下几个方面入手解决方案： 1、减少废弃的数据连接对象的产生和堆积。 2、优化full gc时间. 【调整hikari参数】 可以考虑设置 maxLifetime 为一个较大的值，用于延长连接的生命周期，减少产生被废弃的数据库连接的频率，等到下次 full gc 的时候需要清理的数据库连接对象会大大减少。 Hikari 推荐 maxLifetime 设置为比数据库的 wait_timeout 时间少 30s 到 1min。如果使用的是 mysql 数据库，可以使用 show global variables like ‘%timeout%’; 查看 wait_timeout，腾讯云默认为 1 小时。 下面开始验证，设置maxLifetime = 55分钟，其他条件不变。压测启动前观察jvisualvm，connectionPhantomRefs对象数量 1000s后，观察 connectionPhantomRefs对象 看看connectionPhantomRefs对象没有发生堆积 同时另外注意：minimumIdle和maximumPoolSize不要设置得太大，一般来说配置minimumIdle=10，maximumPoolSize=10～20即可。 我们这次问题产生的根源是数据库连接对象堆积，导致full gc时间过长。解决思路可以从以下三点入手： 1、调整hikari配置参数。例如把maxLifetime设置为较大的值（比数据库的wait_timeout少30s），minimumIdle和maximumPoolSize值不能设置太大，或者直接采用默认值即可。 2、采用G1垃圾回收器？（G1可以自定义STW时间，但R大推荐8g为分界线，8g以下CMS比较推荐）。 3、建立巡查系统，在业务低峰期主动触发full gc。","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"问题集锦","slug":"架构之路/问题集锦","permalink":"https://flyingglass.github.io/categories/架构之路/问题集锦/"}],"tags":[]},{"title":"ConcurrentHashMap源码解析","slug":"ConcurrentHashMap源码解析","date":"2020-06-30T05:59:18.000Z","updated":"2023-03-29T03:21:34.442Z","comments":true,"path":"2020/06/30/ConcurrentHashMap源码解析/","link":"","permalink":"https://flyingglass.github.io/2020/06/30/ConcurrentHashMap源码解析/","excerpt":"","text":"ConcurrentHashMap为Java中常用的并发容器，用于解决HashTable锁住整个hash表的问题，JDK8针对ConcurrentHashMap作了改进和优化，摒弃JDK7的分段锁机制，采用Node + CAS + synchronized保证并发安全。 源码分析 类视图 类注释 类常量 插入 总结","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Java","slug":"架构之路/Java","permalink":"https://flyingglass.github.io/categories/架构之路/Java/"}],"tags":[]},{"title":"HashMap源码解析","slug":"HashMap源码解析","date":"2020-06-30T03:13:45.000Z","updated":"2023-03-29T03:26:08.491Z","comments":true,"path":"2020/06/30/HashMap源码解析/","link":"","permalink":"https://flyingglass.github.io/2020/06/30/HashMap源码解析/","excerpt":"","text":"HashMap是Java中极其频繁的、非常重要的一个集合类，在JDK8改动也比较大，本文主要基于JDK8下HashMap的实现 源码分析 类视图 类注释 允许NULL值，NULL键 不要轻易改变负载因子，负载因子过高会导致链表过长，查找键值对时间复杂度就会增高，负载因子过低会导致hash桶的 数量过多，空间复杂度会增高 Hash表每次会扩容长度为以前的2倍 HashMap是多线程不安全的，在JDK1.7进行多线程put操作，之后遍历，直接死循环，CPU飙到100%，在JDK 1.8中进行多线程操作会出现节点和value值丢失，为什么JDK1.7与JDK1.8多线程操作会出现很大不同，是因为JDK 1.8的作者对resize方法进行了优化不会产生链表闭环。这也是本章的重点之一，具体的细节大家可以去查阅资料。这里就不解释太多了 类常量 12345678910111213141516171819202122232425262728293031 /** * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8; /** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; 构造函数 12345678910111213141516171819202122232425public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; //下面介绍一下这行代码的作用 this.threshold = tableSizeFor(initialCapacity);&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);&#125; HashMap有4个构造函数. 重点介绍下tableSizeFor(initialCapacity)方法，该方法作用，将你传入的initialCapacity进行计算，返回一个大于等于initialCapacity最小的2的幂次方，比如输入6，结算结果为8，源码如下: 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 插入源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //当table为空时，这里初始化table，不是通过构造函数初始化，而是在插入时通过扩容初始化，有效防止了初始化HashMap没有数据插入造成空间浪费可能造成内存泄露的情况 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //存放新键值对 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //旧键值对的覆盖 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //在红黑树中查找旧键值对更新 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //将新键值对放在链表的最后 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //当链表的长度大于等于树化阀值，并且hash桶的长度大于等于MIN_TREEIFY_CAPACITY，链表转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //链表中包含键值对 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //map中含有旧key，返回旧值 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //map调整次数加1 ++modCount; //键值对的数量达到阈值需要扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 上述代码总结如下： 首次插入进行hash表的初始化操作，扩容初始化，插入键值对 插入的键值对中key已经存在，更新键值对 插入链表，如果链表长度大于MIN_TREEIFY_CAPACITY（默认值为8），转化为红黑树，否则直接插入 检查是否需要扩容，当键值对的个数大于threshold阈值进行扩容操作，其中threshold=size*loadFactor 扩容 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //如果旧hash桶不为空 if (oldCap &gt; 0) &#123; //超过hash桶的最大长度，将阀值设为最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; //新的hash桶的长度2被扩容没有超过最大长度，将新容量阀值扩容为以前的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果hash表阈值已经初始化过 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //如果旧hash桶，并且hash桶容量阈值没有初始化，那么需要初始化新的hash桶的容量和新容量阀值 else &#123; newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //新的局部变量阀值赋值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //为当前容量阀值赋值 threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) //初始化hash桶 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //如果旧的hash桶不为空，需要将旧的hash表里的键值对重新映射到新的hash桶中 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //只有一个节点，通过索引位置直接映射 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是红黑树，需要进行树拆分然后映射 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; //如果是多个节点的链表，将原链表拆分为两个链表，两个链表的索引位置，一个为原索引，一个为原索引加上旧Hash桶长度的偏移量 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; //链表1 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //链表2 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //链表1存于原索引 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //链表2存于原索引加上原hash桶长度的偏移量 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 如下情况会产生扩容操作： 初始化HashMap，第一次进行put操作 当键值对的个数大于threshold阀值时产生扩容，threshold=size*loadFactor 源码中关于红黑树的操作、旋转、着色，这里不做介绍，有兴趣可以另行查看 总结 HashMap允许NULL值，NULL键 不要轻易改变负载因子，负载因子过高会导致链表过长，查找键值对时间复杂度就会增高，负载因子过低会导致hash桶的数量过多，空间复杂度会增高 Hash表每次会扩容长度为以前的2倍 HashMap是多线程不安全的，我在JDK 1.7进行多线程put操作，之后遍历，直接死循环，CPU飙到100%，在JDK 1.8中进行多线程操作会出现节点和value值丢失，为什么JDK1.7与JDK1.8多线程操作会出现很大不同，是因为JDK 1.8的作者对resize方法进行了优化不会产生链表闭环。这也是本章的重点之一，具体的细节大家可以去查阅资料。这里就不解释太多了 尽量设置HashMap的初始容量，尤其在数据量大的时候，防止多次resize HashMap在JDK 1.8在做了很好性能的提升，我看到过在JDK1.7和JDK1.8 get操作性能对比JDK1.8是要优于JDK 1.7的，大家感兴趣的可以自己做个测试。","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"Java","slug":"架构之路/Java","permalink":"https://flyingglass.github.io/categories/架构之路/Java/"}],"tags":[]},{"title":"分片和一致性","slug":"分片和一致性","date":"2019-12-25T07:52:42.000Z","updated":"2023-03-29T03:19:54.958Z","comments":true,"path":"2019/12/25/分片和一致性/","link":"","permalink":"https://flyingglass.github.io/2019/12/25/分片和一致性/","excerpt":"","text":"最近整理总结了分布式常用数据结构和算法，特此记录下常用的分布式分片算法和一致性问题。 数据分片与路由哈希分片(Hash Partition)一种数据分片和路由的通用模型，可以将其看作是一个二级映射关系。第一级映射是key-partition映射，其将数据记录映射到数据分片空间，这往往是多对一的映射关系，即一个数据分片包含多条记录数据；第二级映射是partition-machine映射，其将数据分片映射到物理机中，这一般也是多对一映射关系，即一台物理机容纳多个数据分片。 Round Robin 哈希取模法。H(key) = hash(key) mod K，如果物理机增加1台，则数据和物理机之间的映射关系全被打乱。 该方法缺乏扩展灵活性，原因是该方法将物理机和数据分片两个功能点合二为一，即每台物理机对应一个数据分片，这样key-paitition映射和partition-machine映射也就两位一体。 虚拟桶(Virtual Buckets) 所有记录先通过哈希函数映射到对应的虚拟桶，记录和虚拟桶是多对一的映射关系，即一个虚拟桶包含多条记录信息；第二层映射是虚拟桶和物理机之间的映射关系，同样也是多对一映射，一个物理机可以容纳多个虚拟桶，其具体实现是通过查表实现。 当加入新机器，将某些虚拟桶从原来分配的机器重新分配给新机器，只需修改partition-machine映射表中受影响的个别条目就能实现扩展。 一致性哈希(Consistent Hashing) 将哈希数值空间按照大小组成一个首尾相接的环状序列。对于每台机器，可以根据其ip和端口号经过哈希函数映射到哈希数值空间内，这样不同的机器就成了环状序列中的不同节点，而这台机器则负责存储落在一段有序哈希空间内的数据。 路由问题：沿着有向环顺序查找，效率低；为加快查找速度，可以在每个机器节点配置路由表。 数据一致性1. 基本原则 CAP 强一致性(Consisitency)：分布式系统中同一数据多副本情形下，对于数据的更新操作体现出的效果与只有单份数据是一样的 可用性(Availability)：客户端在任何时刻对大规模数据系统的读/写操作都应该保证在限定延时内完成 分区容忍性(Partition Tolerance)：分区间的机器无法进行网络通信的情况 BASE原则 基本可用(Basically Available)：大多数情况下系统可用，允许偶尔的失败 软状态或柔性状态(Soft State)：指数据状态不要求在任何时刻都完全保持同步 最终一致性(Eventual Consistency)：一种弱一致性，不要求任意时刻数据保持一致同步，但是要求在给定时间窗口内数据会达到一致状态 2. 副本更新策略 同时更新 多副本同时更新 主从式更新 对数据的更新操作首先提交到主副本，再由主副本通知从副本更新 任意节点更新 数据更新请求可能发给多副本中任意一个节点，再由这个节点来负责通知其他副本进行更新 3. 一致性协议 两阶段提交(Tow-Phrase Commit, 2PC) - 表决阶段 - 提交阶段 - 如果协调者崩溃，则参与者会存在长时间阻塞的可能 三阶段提交 - 将2PC的提交阶段再次分为两个阶段：预提交阶段和提交阶段，用于解决2PC长时间阻塞的问题。 - 实际使用很少，一方面是2PC发生阻塞情况很少；另一方面是3PC效率过低。 Paxos和Raft一致性协议。 参考链接 分布式系统的事务处理","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"问题集锦","slug":"架构之路/问题集锦","permalink":"https://flyingglass.github.io/categories/架构之路/问题集锦/"}],"tags":[]},{"title":"缓存一致性","slug":"缓存一致性","date":"2019-12-25T06:57:28.000Z","updated":"2023-03-29T03:20:01.456Z","comments":true,"path":"2019/12/25/缓存一致性/","link":"","permalink":"https://flyingglass.github.io/2019/12/25/缓存一致性/","excerpt":"","text":"简介 本文转自：http://coolshell.cn/articles/17416.html 。文章是耗子大神关于缓存更新策略的总结，值得大家学习讨论。 背景看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。 这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设更新数据库和更新缓存都可以成功的情况（我们先把成功的代码逻辑先写对）。 更新缓存的的Design Pattern有四种：Cache aside, Read through, Write through, Write behind caching，我们下面一一来看一下这四种Pattern。 Cache Aside Pattern这是最常用最常用的pattern了。其具体逻辑如下： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。 这是标准的design pattern，包括Facebook的论文《Scaling Memcache at Facebook》也使用了这个策略。为什么不是写完数据库后更新缓存？你可以看一下Quora上的这个问答《Why does Facebook use delete to remove the key-value pair in Memcached instead of updating the Memcached during write request to the backend?》，主要是怕两个并发的写操作导致脏数据。 那么，是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。 但，这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。 所以，这也就是Quora上的那个答案里说的，要么通过2PC或是Paxos协议保证一致性，要么就是拼命的降低并发时脏数据的概率，而Facebook使用了这个降低概率的玩法，因为2PC太慢，而Paxos太复杂。当然，最好还是为缓存设置上过期时间。 Read/Write Through Pattern我们可以看到，在上面的Cache Aside套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而Read/Write Through套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。 Read Through Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。 Write Through Write Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作） 下图自来Wikipedia的Cache)词条。其中的Memory你可以理解为就是我们例子里的数据库。 Write Behind Caching PatternWrite Behind 又叫 Write Back。一些了解Linux操作系统内核的同学对write back应该非常熟悉，这不就是Linux文件系统的Page Cache的算法吗？是的，你看基础这玩意全都是相通的。 所以，基础很重要，我已经不是一次说过基础很重要这事了。 Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 Write Back套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道Unix/Linux非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性能是有冲突的。软件设计从来都是取舍Trade-Off。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 在wikipedia上有一张write back的流程图，基本逻辑如下： 再多唠叨一些1）上面讲的这些Design Pattern，其实并不是软件架构里的mysql数据库和memcache/redis的更新策略，这些东西都是计算机体系结构里的设计，比如CPU的缓存，硬盘文件系统中的缓存，硬盘上的缓存，数据库中的缓存。基本上来说，这些缓存更新的设计模式都是非常老古董的，而且历经长时间考验的策略，所以这也就是，工程学上所谓的Best Practice，遵从就好了。 2）有时候，我们觉得能做宏观的系统架构的人一定是很有经验的，其实，宏观系统架构中的很多设计都来源于这些微观的东西。比如，云计算中的很多虚拟化技术的原理，和传统的虚拟内存不是很像么？Unix下的那些I/O模型，也放大到了架构里的同步异步的模型，还有Unix发明的管道不就是数据流式计算架构吗？TCP的好些设计也用在不同系统间的通讯中，仔细看看这些微观层面，你会发现有很多设计都非常精妙……所以，请允许我在这里放句观点鲜明的话——如果你要做好架构，首先你得把计算机体系结构以及很多老古董的基础技术吃透了。 3）在软件开发或设计中，我非常建议在之前先去参考一下已有的设计和思路，看看相应的guideline，best practice或design pattern，吃透了已有的这些东西，再决定是否要重新发明轮子。千万不要似是而非地，想当然的做软件设计。 4）上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新Cache成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback，比如Java 7 的XAResource，还有MySQL 5.7的 XA Transaction，有些cache也支持XA，比如EhCache。当然，XA这样的强一致性的玩法会导致性能下降，关于分布式的事务的相关话题，你可以看看《分布式系统的事务处理》一文。 （全文完） 参考链接 缓存更新的套路","categories":[{"name":"架构之路","slug":"架构之路","permalink":"https://flyingglass.github.io/categories/架构之路/"},{"name":"问题集锦","slug":"架构之路/问题集锦","permalink":"https://flyingglass.github.io/categories/架构之路/问题集锦/"}],"tags":[]},{"title":"ssr开机启动","slug":"ssr开机启动","date":"2019-06-19T07:56:47.000Z","updated":"2021-05-06T09:51:07.439Z","comments":true,"path":"2019/06/19/ssr开机启动/","link":"","permalink":"https://flyingglass.github.io/2019/06/19/ssr开机启动/","excerpt":"","text":"本文主要介绍privoxy和ssr的安装和开机启动，运行系统为centos7.6，python版本为2.7.x；至于为何使用ssr，而不使用ss，主要得益于ssr具有的加密和混淆特性，其中的爱恨纠葛就不赘述，有兴趣的小伙伴可以自行搜索。 SSR Client123456789101112131415161718192021222324252627282930313233343536373839404142git clone git@github.com:shadowsocksr-backup/shadowsocksr.git# 超链接cd /usr/local/bin &amp;&amp; ln -s $&#123;pwd&#125;/shadowsocksr/shadowsocks/local.py ssrlocal# 配置shadowsocks.jsoncat /etc/shadowsocks.json &#123; \"server\":\"your server ip\", \"server_ipv6\": \"::\", \"server_port\":8388, \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"password\":\"your password\", \"timeout\":300, \"udp_timeout\": 60, \"method\":\"aes-256-cfb\", \"protocol\": \"auth_aes128_md5\", \"protocol_param\": \"\", \"obfs\":\"tls1.2_ticket_auth\", \"obfs_param\": \"\", \"fast_open\": false, \"workers\": 1&#125;# 配置启动脚本vim /etc/systemd/system/shadowsocksr.service[Unit]Description=SSR[Service]TimeoutStartSec=0ExecStart=/usr/local/bin/ssrlocal -c /etc/shadowsocks.json[Install]WantedBy=multi-user.target# 设置开机启动systemctl enable shadowsocksr# 启动systemctl start shadowsocksr# 查看systemctl status shadowsocksr# 停止systemctl stop shadowsocksr# 取消开机启动systemctl disable shadowsocksr Privoxy123456789101112131415161718yum install -y privoxy# 修改配置cat /etc/privoxy/config | grep -v \"#\"listen-address 0.0.0.0:8118...forward-socks5 / 127.0.0.1:1080 .# 设置开机启动systemctl enable privoxy# 启动systemctl start privoxy# 查看systemctl status privoxy# 停止systemctl stop privoxy# 取消开机启动systemctl disable privoxy 参考链接 Centos + Shadowsocks客户端 + Privoxy实现外网访问 windows ssr SSR-ShadowsocksR-配置及使用","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"docker常用命令","slug":"docker常用命令","date":"2019-06-17T08:06:33.000Z","updated":"2021-05-06T09:59:14.871Z","comments":true,"path":"2019/06/17/docker常用命令/","link":"","permalink":"https://flyingglass.github.io/2019/06/17/docker常用命令/","excerpt":"","text":"docker常用命令1234567891011121314151617181920212223242526272829303132# 清理无效的imagesdocker rmi $(docker images -a -q)# daemon启动指定的imagedocker run -d fcoin:latest# 进入container容器docker exec -ti e7e325e08354 sh# build 镜像docker build -t fcoin:latest -f Dockerfile .# 停止containerdocker stop e7e325e08354# 指定docker-compose.yml，设置project name，强制builddocker-compose -f docker-compose.yml -p docker up --build# 查看日志docker-compose logs -f# 后台启动docker-compose up -d# 停止docker-compose down# 常用配置cat /etc/docker/daemon.json&#123; \"insecure-registries\": [ \"doc.flyflyfish.com:8082\" ], \"registry-mirrors\": [ \"http://doc.flyflyfish.com:8082\" ], \"debug\": true, \"experimental\": false&#125;","categories":[{"name":"Docker和K8s","slug":"Docker和K8s","permalink":"https://flyingglass.github.io/categories/Docker和K8s/"}],"tags":[]},{"title":"rpm离线安装脚本","slug":"rpm离线安装脚本","date":"2019-06-17T07:48:43.000Z","updated":"2021-05-06T09:51:04.002Z","comments":true,"path":"2019/06/17/rpm离线安装脚本/","link":"","permalink":"https://flyingglass.github.io/2019/06/17/rpm离线安装脚本/","excerpt":"","text":"rpm离线安装脚本，主要从https://pkgs.org/下载 下载脚本123456789#!/usr/bin/env bashRPM_ARRAY=(perl-5.16.3-293.el7.x86_64.rpm perl-Carp-1.26-244.el7.noarch.rpm perl-Encode-2.51-7.el7.x86_64.rpm perl-Exporter-5.68-3.el7.noarch.rpm perl-File-Path-2.09-2.el7.noarch.rpm perl-File-Temp-0.23.01-3.el7.noarch.rpm perl-Filter-1.49-3.el7.x86_64.rpm perl-Getopt-Long-2.40-3.el7.noarch.rpm perl-HTTP-Tiny-0.033-3.el7.noarch.rpm perl-PathTools-3.40-5.el7.x86_64.rpm perl-Pod-Escapes-1.04-293.el7.noarch.rpm perl-Pod-Perldoc-3.20-4.el7.noarch.rpm perl-Pod-Simple-3.28-4.el7.noarch.rpm perl-Pod-Usage-1.63-3.el7.noarch.rpm perl-Scalar-List-Utils-1.27-248.el7.x86_64.rpm perl-Socket-2.010-4.el7.x86_64.rpm perl-Storable-2.45-3.el7.x86_64.rpm perl-Text-ParseWords-3.29-4.el7.noarch.rpm perl-Time-HiRes-1.9725-3.el7.x86_64.rpm perl-Time-Local-1.2300-2.el7.noarch.rpm perl-constant-1.27-2.el7.noarch.rpm perl-libs-5.16.3-293.el7.x86_64.rpm perl-macros-5.16.3-293.el7.x86_64.rpm perl-parent-0.225-244.el7.noarch.rpm perl-podlators-2.5.1-3.el7.noarch.rpm perl-threads-1.87-4.el7.x86_64.rpm perl-threads-shared-1.43-6.el7.x86_64.rpm)for val in $&#123;RPM_ARRAY&#125;; do echo $&#123;val&#125; wget http://mirror.centos.org/centos/7/os/x86_64/Packages/$&#123;val&#125;done 参考链接 bash_loop_list_strings","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"mysql5.7离线安装","slug":"mysql5-7离线安装","date":"2019-06-17T02:53:49.000Z","updated":"2021-05-06T09:45:43.935Z","comments":true,"path":"2019/06/17/mysql5-7离线安装/","link":"","permalink":"https://flyingglass.github.io/2019/06/17/mysql5-7离线安装/","excerpt":"","text":"本文主要介绍mysql5.7的离线安装，关于mysql的配置可以参考之前的博客Mysql配置，离线安装中需要的rpm包可以前往https://pkgs.org/下载 Centos 7.5 下载安装mysql，下载mysql的安装包，下载地址，解压后rpm列表如下： 123456789101112tar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar# lsmysql-community-client-5.7.26-1.el7.x86_64.rpmmysql-community-common-5.7.26-1.el7.x86_64.rpmmysql-community-devel-5.7.26-1.el7.x86_64.rpmmysql-community-embedded-5.7.26-1.el7.x86_64.rpmmysql-community-embedded-compat-5.7.26-1.el7.x86_64.rpmmysql-community-embedded-devel-5.7.26-1.el7.x86_64.rpmmysql-community-libs-5.7.26-1.el7.x86_64.rpmmysql-community-libs-compat-5.7.26-1.el7.x86_64.rpmmysql-community-server-5.7.26-1.el7.x86_64.rpmmysql-community-test-5.7.26-1.el7.x86_64.rpm 依赖包 numactl软件包 123numactl-2.0.9-4.el7_2.x86_64.rpmnumactl-devel-2.0.9-4.el7_2.x86_64.rpmnumactl-libs-2.0.9-4.el7_2.x86_64.rpm perl软件包 1perl-Data-Dumper-2.145-3.el7.x86_64.rpm perl相关 123456789101112131415161718192021222324252627perl-5.16.3-293.el7.x86_64.rpm perl-Carp-1.26-244.el7.noarch.rpmperl-Encode-2.51-7.el7.x86_64.rpmperl-Exporter-5.68-3.el7.noarch.rpmperl-File-Path-2.09-2.el7.noarch.rpmperl-File-Temp-0.23.01-3.el7.noarch.rpmperl-Filter-1.49-3.el7.x86_64.rpmperl-Getopt-Long-2.40-3.el7.noarch.rpmperl-HTTP-Tiny-0.033-3.el7.noarch.rpmperl-PathTools-3.40-5.el7.x86_64.rpmperl-Pod-Escapes-1.04-293.el7.noarch.rpmperl-Pod-Perldoc-3.20-4.el7.noarch.rpmperl-Pod-Simple-3.28-4.el7.noarch.rpmperl-Pod-Usage-1.63-3.el7.noarch.rpmperl-Scalar-List-Utils-1.27-248.el7.x86_64.rpmperl-Socket-2.010-4.el7.x86_64.rpmperl-Storable-2.45-3.el7.x86_64.rpmperl-Text-ParseWords-3.29-4.el7.noarch.rpmperl-Time-HiRes-1.9725-3.el7.x86_64.rpmperl-Time-Local-1.2300-2.el7.noarch.rpmperl-constant-1.27-2.el7.noarch.rpmperl-libs-5.16.3-293.el7.x86_64.rpmperl-macros-5.16.3-293.el7.x86_64.rpmperl-parent-0.225-244.el7.noarch.rpmperl-podlators-2.5.1-3.el7.noarch.rpmperl-threads-1.87-4.el7.x86_64.rpmperl-threads-shared-1.43-6.el7.x86_64.rpm 其他软件包 libaio-0.3.109-13.el7.x86_64.rpm 查询并卸载系统自带的Mariadb（会与mysql冲突） 123# 查询mariadbrpm -ga | grep mariadbrpm -e --nodeps 查询出来的版本 安装mysql 1234rpm -ivh libaio-0.3.109-13.el7.x86_64.rpmrpm -ivh numactl*rpm -ivh perl-*rpm -ivh mysql-community-* 启动mysql服务 1systemctl start mysqld 设置开机启动 123systemctl enable mysqldsystemctl daemon-reload 查看mysql root临时密码 1234cat /var/log/mysqld.log | grep password# 修改密码# mysql -uroot -p# SET PASSWORD FOR ‘root’@’localhost’ = PASSWORD(‘newpass’); 新建用户并授权 123create user newuser@’%’ identified by ‘password’;grant all privileges on db.* to newuser;# grant all privileges on *.* to newuser; 参考链接 Centos7离线安装MySql","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"ubuntu移动mysql的datadir","slug":"ubuntu移动mysql的datadir","date":"2018-11-26T10:06:20.000Z","updated":"2021-05-06T09:51:14.292Z","comments":true,"path":"2018/11/26/ubuntu移动mysql的datadir/","link":"","permalink":"https://flyingglass.github.io/2018/11/26/ubuntu移动mysql的datadir/","excerpt":"","text":"Introduction数据库会随着时间的推移而增长，有时会超出文件系统上的空间。当它们位于与操作系统其余部分相同的分区上时，您还可能遇到I/O争用。RAID、网络块存储和其他设备可以提供冗余和其他需要的特性。无论您是在添加更多空间、评估优化性能的方法，还是希望利用其他存储特性，本文都将指导您重新定位MySQL的数据目录。 Prerequisites前置条件: ubuntu16.04服务器，具有非根用户和sudo特权。 MySQL服务器。如果你还没有安装MySQL, 可以参考之前博文的mysql的安装。 STEP 登录mysql查看当前的datadir： 1234mysql -u root -pmysql&gt; select @@datadir;sudo systemctl stop mysqlsudo systemctl status mysql 同步现有的datadir到新的目录： 12345678910111213141516# rsyncsudo rsync -av /var/lib/mysql /mnt/data01sudo mv /var/lib/mysql /var/lib/mysql.baksudo vim /etc/mysql/mysql.conf.d/mysqld.cnf$ cat /etc/mysql/mysql.conf.d/mysqld.cnf...datadir = /mnt/data01/mysql...# Configuring AppArmor Access Control Rulessudo vim /etc/apparmor.d/tunables/alias$ cat /etc/apparmor.d/tunables/alias. . .alias /var/lib/mysql/ -&gt; /mnt/data01/mysql/. . .# Restart apparmorsudo systemctl restart apparmor 创建目录以通过/usr/share/mysql/mysql-systemd-start脚本的检测： 123456789101112131415$ cat /usr/share/mysql/mysql-systemd-start...datadir=$(get_mysql_option mysqld datadir \"/var/lib/mysql\")if [ ! -d \"$&#123;datadir&#125;\" ] &amp;&amp; [ ! -L \"$&#123;datadir&#125;\" ]; then echo \"MySQL data dir not found at $&#123;datadir&#125;. Please create one.\" exit 1fi if [ ! -d \"$&#123;datadir&#125;/mysql\" ] &amp;&amp; [ ! -L \"$&#123;datadir&#125;/mysql\" ]; then echo \"MySQL system database not found in $&#123;datadir&#125;. Please run mysqld --initialize.\" exit 1fi...# 创建目录，根据自己的datadir决定sudo mkdir /var/lib/mysql/mysql -p 验证是否更改成功 12345678910111213sudo systemctl start mysqlsudo systemctl status mysqlmysql -u root -pmysql&gt; select @@datadir;+----------------------------+| @@datadir |+----------------------------+| /mnt/data01/mysql/ |+----------------------------+1 row in set (0.01 sec)# Restart Mysqlsudo systemctl restart mysqlsudo systemctl status mysql 参考链接： How To Install MySQL on Ubuntu 16.04 how-to-move-a-mysql-data-directory-to-a-new-location-on-ubuntu-16-04","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"eth的gas估算","slug":"eth的gas估算","date":"2018-06-15T03:00:06.000Z","updated":"2021-04-29T09:45:31.260Z","comments":true,"path":"2018/06/15/eth的gas估算/","link":"","permalink":"https://flyingglass.github.io/2018/06/15/eth的gas估算/","excerpt":"","text":"eth平台消耗的Gas满足如下公式$cost = gasUsage * gasPrice $ $gasUsage为gas的消耗量$ $gasPrice为gas的单价，与交易打包确认时间负相关$ 由于gasPrice主要影响交易等待打包确认时间，由eth平台繁忙程度所影响，可以查看参考链接中关于gasPrice 和交易等待时间的影响结合自身的业务决定，故本文只着重介绍普通交易和智能合约运行所需的gasUsage的估算，在计算最终cost时，统一采用$gasPrice = 1 Gwei$ 普通交易所需的Gas在eth平台执行普通的交易转账需要消耗gas，消耗的gas 满足一下公式: $cost = gasUsage * gasPrice = (账户发起者减少的资产 - 账户接收者增加的资产)$ 其中$gasUsage为固定值=21000$ $gasPrice为gas的单价，与交易打包确认时间负相关$ 智能合约所需的Gas普通转账交易所需的gas固定值为21000，调用智能合约方法所需要的gas并不一定,主要由占用的资源(cpu，内存等)决定（计算量影响cpu），占用的资源越多，所需要的gas也越多。我们主要分析两个智能合约的gas消耗估算： 智能合约token转账 采用truffle demo的MetaCoin.sol进行智能合约的测试，合约内容如下： 12345678910111213141516171819202122232425262728pragma solidity ^0.4.2;import &quot;./ConvertLib.sol&quot;;contract MetaCoin &#123; mapping (address =&gt; uint) balances; event Transfer(address indexed _from, address indexed _to, uint256 _value); function MetaCoin() &#123; balances[tx.origin] = 10000; &#125; // 转账逻辑 function sendCoin(address receiver, uint amount) returns(bool sufficient) &#123; if (balances[msg.sender] &lt; amount) return false; balances[msg.sender] -= amount; balances[receiver] += amount; Transfer(msg.sender, receiver, amount); return true; &#125; function getBalanceInEth(address addr) returns(uint)&#123; return ConvertLib.convert(getBalance(addr),2); &#125; function getBalance(address addr) returns(uint) &#123; return balances[addr]; &#125;&#125; 使用以上智能合约，进行一次转账操作(Send Meta Token,发送的token数量为1)： 1234Transaction: 0xc3591a4a8614721ce176f58782455b127d1946dbcf10876c7e7c4fa169da849bGas usage: 36050Block Number: 7Block Time: Fri Jun 15 2018 11:05:41 GMT+0800 (CST) 通过testrpc日志可以看到Gas usage为36050 智能合约存储 测试一下简单的智能合约SimpleStorage.sol 123456789101112pragma solidity ^0.4.2; contract SimpleStorage &#123; string storedData; function set(string x) &#123; storedData = x; &#125; function get() constant returns (string) &#123; return storedData; &#125; &#125; 在truffle console执行智能合约contract.set(&#39;8DFCB2DA63B35B9DAA0906FCA18053FD&#39;)存入32 ASCII Character 123456789101112&#123; tx: '0xb49d7d757920514d5ef9f2bbb5c1171d76d9bcfa8e4633f05f73c718164812f3', receipt: &#123; transactionHash: '0xb49d7d757920514d5ef9f2bbb5c1171d76d9bcfa8e4633f05f73c718164812f3', transactionIndex: 0, blockHash: '0x84f864f4999edd4278a714af9ba3c6ea99010c8ef2ce45551e93ad019ae4c03d', blockNumber: 8, gasUsed: 64764, cumulativeGasUsed: 64764, contractAddress: null, logs: [], status: 1 &#125;, logs: [] &#125; 通过testrpc日志可以看到Gas usage为64764(不同的机器环境会有些误差) 单位换算$ 1eth = 10^9 Gwei = 10^{18} wei = $ $508.08 比如普通转账的gas估算： $gasPrice = 1Gwei$ $gasUsage = 21000$ cost = gasUsage * gasPrice = 21000 * 1Gwei / 10 ^{18}wei = 2.1 * 10 ^ {-5}eth = $ 0.01 参考链接 gasPrice和打包确认时间实时信息","categories":[{"name":"区块链","slug":"区块链","permalink":"https://flyingglass.github.io/categories/区块链/"}],"tags":[]},{"title":"aws配置eb-cli","slug":"aws配置eb-cli","date":"2018-06-02T04:25:02.000Z","updated":"2021-05-06T09:47:54.305Z","comments":true,"path":"2018/06/02/aws配置eb-cli/","link":"","permalink":"https://flyingglass.github.io/2018/06/02/aws配置eb-cli/","excerpt":"","text":"EB CLI 是 Elastic Beanstalk 的命令行界面，它提供了可简化从本地存储库创建、更新和监控环境的交互式命令。将 EB CLI 用作每日开发和测试周期的一部分，取代 AWS 管理控制台。 Prerequisites Larger than Python 2.7 or Python3.4 安装EB CLI1234567# 可选参数 --upgrade更新依赖组件 --user安装到用户子目录pip install awsebcli # 设置Env, eg: LOCAL_PATH=~/.local/binexport PATH=LOCAL_PATH:$PATHsource ~/.bashrc# Check eb versioneb --version 配置 EB CLI12345678910111213# Maven package 激活pro的profile打入ebextensions的nginx配置cd my-app &amp;&amp; mvnw clean package -Dmaven.test.skip=true# EB initeb init# EB create env profileeb create# EB deployeb deploy# EB logseb logs# 登录 EB Instance 检查详细日志# eb ssh # tailf /var/log/nginx/access.log 配置示例(cat .elasticbeanstalk/config.yml)123456789101112131415161718192021222324branch-defaults: master: environment: my-app-dev group_suffix: null release: environment: my-app-dev group_suffix: nulldeploy: # 需要上传java包的路径 artifact: target/my-app.warglobal: application_name: my-app branch: null default_ec2_keyname: mypem default_platform: Java 8 default_region: us-east-1 include_git_submodules: true instance_profile: null platform_name: null platform_version: null profile: eb-cli repository: null sc: git workspace_type: Application 反向代理配置12345678910#### Nginx配置如何生效- 目录结构如下： my-app.war |-- .ebextensions | `-- nginx | `nginx.conf | `agent_deny `-- org `--META-INF `--WEB-INF Pom.xml1234567891011121314151617181920212223242526&lt;!--maven package springboot war--&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;finalName&gt;$&#123;project.artifactId&#125;&lt;/finalName&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;failOnMissingWebXml&gt;false&lt;/failOnMissingWebXml&gt; &lt;webResources&gt; &lt;resource&gt; &lt;directory&gt;src/main/ebextensions&lt;/directory&gt; &lt;targetPath&gt;.ebextensions&lt;/targetPath&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/webResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 参考链接 安装 Elastic Beanstalk 命令行界面 (EB CLI) 配置 EB CLI 配置反向代理","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"nginx反爬策略配置","slug":"nginx反爬策略配置","date":"2018-06-02T03:39:00.000Z","updated":"2021-05-06T09:57:55.021Z","comments":true,"path":"2018/06/02/nginx反爬策略配置/","link":"","permalink":"https://flyingglass.github.io/2018/06/02/nginx反爬策略配置/","excerpt":"","text":"网站部署上线后，为了防止爬虫的侵扰，一般会进行反爬策略限制。在Web开发中Nginx一般作为Gateway和Router，本文主要介绍如何使用Nginx进行User-Agent和RateLimit两个方面来进行反爬 User-Agentcat /etc/nginx/agent_deny 1234567891011121314#禁止Scrapy等工具的抓取if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) &#123; return 403;&#125;#禁止指定UA及UA为空的访问if ($http_user_agent ~* \"FeedDemon|Indy Library|Alexa Toolbar|AskTbFXTV|AhrefsBot|CrawlDaddy|CoolpadWebkit|Java|Feedly|UniversalFeedParser|ApacheBench|Microsoft URL Control|Swiftbot|ZmEu|oBot|jaunty|Python-urllib|lightDeckReports Bot|YYSpider|DigExt|HttpClient|MJ12bot|heritrix|EasouSpider|Ezooms|^$\" ) &#123; return 403;&#125;#禁止非GET|POST方式的抓取if ($request_method !~ ^(GET|POST)$) &#123; return 403;&#125; RateLimitRateLimit传统的算法主要分为token bucket和leaky bucket，nginx和传统的leaky bucket算法略微有些区别，leaky bucket算法主要处理方式Traffic Shaping和Traffic Policing，Traffic Shaping的核心理念是”等待”，Traffic Policing的核心理念是”丢弃”，在bucket满后，常见的处理方式为： 暂时拦截住上方水的向下流动，等待桶中的一部分水漏走后，再放行上方水 溢出的上方水直接抛弃 12345678910111213141516171819# Rate Limit# 此设置只能放在 http 节点下limit_req_zone $binary_remote_addr zone=ratelimit:10m rate=2r/s;# 默认为503 Service Unavailable，返回429 Too Many Request更为合适limit_req_status 429;# Serverserver &#123; listen 80; server_name localhost; charset utf-8; location / &#123; # 此设置在 http, server, location 节点都可以设置；设置了 nodelay 将不会等待 limit_req zone=ratelimit burst=20 nodelay; include agent_deny; proxy_pass http://127.0.0.1:8080/java-web; &#125;&#125; 参考链接 Nginx下limit_req模块burst参数超详细解析 Nginx模块开发（10）—limit_req模块分析 漏桶算法和 NGINX 的 limit_req 模块","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nginx的https配置","slug":"nginx的https配置","date":"2018-05-27T13:38:26.000Z","updated":"2021-05-06T09:57:50.670Z","comments":true,"path":"2018/05/27/nginx的https配置/","link":"","permalink":"https://flyingglass.github.io/2018/05/27/nginx的https配置/","excerpt":"","text":"在web开发中，一般使用nginx作为网关，本文介绍nginx下的https的配置 证书申请主要介绍阿里云免费SSL证书申请 购买SSL证书：阿里云免费SSL地址 修改CNAME配置：SSL的DNS验证需要在DNS解析规则添加TXT记录，需要删除CNAME或者改为A记录 补全信息：一个免费SSL证书只能绑定一个域名，选择DNS验证 Nginx配置 创建证书目录：mkdir /etc/nginx/cert 进行https配置vim /etc/nginx/sites-available/ssl： 12345678910111213141516server &#123; listen 443; server_name your.domain.com; root /var/www/html; index index.html index.htm; ssl on; ssl_certificate cert/cert.pem; ssl_certificate_key cert/cert.key; ssl_session_timeout 5m; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_prefer_server_ciphers on; location / &#123; try_files $uri $uri/ =404; &#125;&#125; 配置http重定向到https： 12345678# HTTP serverserver &#123; listen 80; server_name your.domain.com; #rewrite ^(.*)$ https://$host$1 permanent; # Nginx 旧写法,$1匹配()的内容 return 301 https://$host$request_uri; # Nginx 新写法&#125; 参考链接 使用阿里云免费SSL证书实现全站HTTPS化 域名验证推送到阿里云DNS失败了，该怎么办？ Nginx的https配置记录以及http强制跳转到https的方法梳理 nginx配置location总结及rewrite规则写法","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nginx反向代理配置","slug":"nginx反向代理配置","date":"2018-05-12T02:48:41.000Z","updated":"2021-05-06T09:57:58.540Z","comments":true,"path":"2018/05/12/nginx反向代理配置/","link":"","permalink":"https://flyingglass.github.io/2018/05/12/nginx反向代理配置/","excerpt":"","text":"在web开发中，nginx一般被用作网关，配置路由功能，在使用nginx进行反向代理的过程中，碰到一些配置问题，特此文记录 由于项目中涉及到php和java的web服务，在使用nginx进行转发过程中，需要通过rewrite和location配合反向代理，配置略微繁琐，废话不多说直接上nginx的配置。 Nginx的配置12345678910111213141516171819location / &#123; # Filename Not Found if (!-e $request_filename) &#123; rewrite \"^/(.*)$\" /index.php last; &#125; # Uri contains php-web url if ( $uri ~ /php-web ) &#123; rewrite ^/$/index.php last; &#125; # Uri not contains php-web url if ( $uri !~ /php-web ) &#123; rewrite / /java-web last; &#125; try_files $uri $uri/ /index.php;&#125;# Filter java-web urilocation ^~ /java-web &#123; proxy_pass http://ip:port/java-web;&#125; 由于nginx没有if else之类的结构，通过多个if结构进行过滤，上述配置的项目实现了: 如果请求的uri中含有php-web将进行php web处理 如果请求的uri中不含有php-web将进行java web处理 参考链接 nginx配置url重写 nginx location 匹配规则","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"jenkins自动化部署","slug":"jenkins自动化部署","date":"2018-05-11T15:32:14.000Z","updated":"2021-05-06T09:57:15.073Z","comments":true,"path":"2018/05/11/jenkins自动化部署/","link":"","permalink":"https://flyingglass.github.io/2018/05/11/jenkins自动化部署/","excerpt":"","text":"目前在部署blog之类的静态网站，每次修改完毕后重新部署，略显繁琐，决定通过jenkins+gitlab来进行自动化部署，至于如何安装jenkins+gitlab不是本文重点，可参考我之前的安装博文 实现自动化的部署原理：jenkins通过捕获gitlab的push event来进行自动部署 Jenkins和Gitlab配置 配置gitlab认证： 路径：Jenkins--&gt;Credentials--&gt;System--&gt;Global credentials(unrestricted)--&gt;Add Credentials Kind选择Gitlab API token 其中API token填写gitlab中有指定仓库权限的账号 ID填写用户账号 Jenkins配置gitlab连接 路径：系统管理—&gt;系统设置 填写连接名，自定义 填写gitlab访问URL 选择gitlab认证 测试连接 Jenkins任务配置 General配置： 选择Gitlab connection：gitlab（上面步骤配置gitlab连接的连接名） 在Gitlab connection填入group/repository name 构建触发器配置： 勾选Build when a change is pushed to GitLab. GitLab CI Service URL:http://ip:port/jenkins/project/repository name 选择push events 事件触发构建 选择分支过滤（此处可以根据不同的需求来使用过滤功能） secret token后面将会填入Gitlab项目中的webhook 添加Gitlab webhook 在gitlab中找到项目—&gt;setting—&gt;Integrations配置 URL：填入触发器中配置的GitLab webhook URL Secret Token:构建器中生成的Secret Token 点击Add webhook 点击test—&gt;选择push event进行测试：Hook excuted successfully：HTTP 200即为成功 构建配置 123456789101112131415161718192021#!/usr/bin/env bashecho $&#123;WORKSPACE&#125;HTML_PATH=/var/www/htmlNGINX_USER=www-dataNGINX=/usr/sbin/nginx# Git archive zipcd $&#123;WORKSPACE&#125; &amp;&amp; git archive --format zip --output \"./output.zip\" -0 HEAD# Delete older and copy newersudo rm -rf $&#123;HTML_PATH&#125; &amp;&amp; sudo mkdir -p $&#123;HTML_PATH&#125; &amp;&amp; sudo mv $&#123;WORKSPACE&#125;/output.zip $&#123;HTML_PATH&#125;# Unzipcd $&#123;HTML_PATH&#125; &amp;&amp; sudo unzip output.zip &amp;&amp; sudo chown -R $NGINX_USER:$NGINX_USER $HTML_PATH # Nginxsudo $NGINX -s reload# Cleansudo rm -rf $&#123;HTML_PATH&#125;/output.zip $&#123;HTML_PATH&#125;/reload-nginx.sh 参考链接 自动化部署之jenkins自动触发构建和发布","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"mybatis多数据源","slug":"mybatis多数据源","date":"2018-05-11T07:50:06.000Z","updated":"2021-05-06T09:53:02.269Z","comments":true,"path":"2018/05/11/mybatis多数据源/","link":"","permalink":"https://flyingglass.github.io/2018/05/11/mybatis多数据源/","excerpt":"","text":"对于多数据源，一般都是问了解决主从模式或者业务相对复杂需要分库来支持业务的场景。搜索解决方案，要么是采用spring多数据源的方案，要么是利用aop动态切换，感觉有点小复杂，本文采用一种相对简单的解决方案，配置过程中遇到一些问题，特以此文记录 配置yml123456789101112131415161718192021222324252627282930313233343536373839############################# Multiple DataSourcemulti: datasource: enabled: true #控制是否开启多数据源 druid: master: url: jdbc:mysql://ip:port/master?useUnicode=true&amp;characterEncoding=utf-8&amp;characterSetResults=utf8&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;useSSL=false username: root password: root driver-class-name: com.mysql.jdbc.Driver slave: url: jdbc:mysql://ip:port/slave?useUnicode=true&amp;characterEncoding=utf-8&amp;characterSetResults=utf8&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;useSSL=false username: root password: root driver-class-name: com.mysql.jdbc.Driver mybatis: master: base-packages: com.fly.mapper.master mapper-locations: classpath:mapper/master/*.xml slave: base-packages: com.fly.mapper.slave mapper-locations: classpath:mapper/slave/*.xml ################################## mybaitsmybatis: type-aliases-package: com.fly.bean.mysql # 配置 configuration: cache-enabled: true # 全局映射器启用缓存 lazy-loading-enabled: true # 查询时，关闭关联对象即时加载以提高性能 aggressive-lazy-loading: false # 设置关联对象加载的形态，此处为按需加载字段(加载字段由SQL指定)，不会加载关联表的所有字段，以提高性能 --&gt; multiple-result-sets-enabled: true # 对于未知的SQL查询，允许返回不同的结果集以达到通用的效果 --&gt; use-column-label: true # 允许使用列标签代替列名 --&gt; use-generated-keys: true # 允许使用自定义的主键值(比如由程序生成的UUID 32位编码作为键值)，数据表的PK生成策略将被覆盖 --&gt; auto-mapping-behavior: full # 给予被嵌套的resultMap以字段-属性的映射支持 --&gt; default-executor-type: batch # 对于批量更新操作缓存SQL以提高性能 --&gt; default-statement-timeout: 25000 # 数据库超过25000秒仍未响应则超时 --&gt; mapUnderscoreToCamelCase: true # mybatis 驼峰命名 --&gt; log-impl: org.apache.ibatis.logging.stdout.StdOutImpl # 命令行输出 --&gt; 配置数据源123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * 多数据源 * 1. mybatis-spring-boot-starter的MybatisAutoConfiguration不支持Multiple DataSource * 仅支持Single DataSource或@Primary DataSource生效 * 2. 多数据源的配置必须要配置主库@Primary否则报错 * 或者@EnableAutoConfiguration(exclude = MybatisAutoConfiguration.class)禁止AutoConfig * 3. 多数据源采用主，副库的配置方式，通过不同包进行隔离 */@org.springframework.context.annotation.Configuration@ConditionalOnProperty(name = \"multi.datasource.enabled\", havingValue = \"true\")public class MultiDataSourceConfig &#123; /** * ******************************************************************* * 主数据源配置 @Primary * ******************************************************************* */ @org.springframework.context.annotation.Configuration @ConditionalOnProperty(name = \"multi.datasource.enabled\", havingValue = \"true\") @MapperScan( basePackages = \"com.fly.mapper.master\", sqlSessionFactoryRef = \"masterSqlSessionFactory\" ) @Getter @Setter @ConfigurationProperties( prefix = \"multi.datasource.mybatis.master\" ) static class MasterDataSourceConfig &#123; @javax.annotation.Resource MybatisConfigurationSupport support; private String[] mapperLocations; /** * Primary Druid DataSource * @return */ @ConfigurationProperties( prefix = \"multi.datasource.druid.master\" ) @Primary @Bean public DataSource masterDataSource() &#123; return DruidDataSourceBuilder .create() .build(); &#125; @Bean @Primary public SqlSessionFactoryBean masterSqlSessionFactory( @Qualifier(\"masterDataSource\") DataSource dataSource ) throws Exception &#123; return support.createSqlSessionFactoryBean(dataSource, mapperLocations); &#125; &#125; /** * ******************************************************************* * 从数据源配置 * ******************************************************************* */ @org.springframework.context.annotation.Configuration @ConditionalOnProperty(name = \"multi.datasource.enabled\", havingValue = \"true\") @MapperScan( basePackages = \"com.fly.mapper.slave\", sqlSessionFactoryRef = \"slaveSqlSessionFactory\" ) @Getter @Setter @ConfigurationProperties( prefix = \"multi.datasource.mybatis.slave\" ) static class SlaveDataSourceConfig &#123; private String[] mapperLocations; @javax.annotation.Resource MybatisConfigurationSupport support; /** * Slave Druid DataSource * @return */ @ConfigurationProperties( prefix = \"multi.datasource.druid.slave\" ) @Bean public DataSource slaveDataSource() &#123; return DruidDataSourceBuilder .create() .build(); &#125; @Bean public SqlSessionFactoryBean slaveSqlSessionFactory( @Qualifier(\"slaveDataSource\") DataSource dataSource ) throws Exception &#123; return support.createSqlSessionFactoryBean(dataSource, mapperLocations); &#125; &#125; /** * ******************************************************************* * 用于简化MybatisProperties的配置 * ******************************************************************* */ @Component @ConditionalOnProperty(name = \"multi.datasource.enabled\", havingValue = \"true\") static class MybatisConfigurationSupport &#123; private final MybatisProperties properties; private final ResourceLoader resourceLoader; public MybatisConfigurationSupport( MybatisProperties properties, ResourceLoader resourceLoader) &#123; this.properties = properties; this.resourceLoader = resourceLoader; &#125; /** * 创建SqlSessionFactoryBean * @param dataSource * @param mapperLocations * @return */ public SqlSessionFactoryBean createSqlSessionFactoryBean(DataSource dataSource, String[] mapperLocations) &#123; SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); factory.setVfs(SpringBootVFS.class); /** * 处理Configuration * 必须Copy否则会抛出配置多个分页插件的异常 */ Configuration configuration = new Configuration(); if (properties.getConfiguration() != null) &#123; // 必须Copy否则会抛出配置多个分页插件的异常 BeanUtils.copyProperties(properties.getConfiguration(), configuration); &#125; factory.setConfiguration(configuration); // 处理MapperLocations if (!ObjectUtils.isEmpty(mapperLocations)) &#123; factory.setMapperLocations(resolveMapperLocations(mapperLocations)); &#125;// if (StringUtils.hasLength(properties.getTypeAliasesPackage())) &#123;// factory.setTypeAliasesPackage(properties.getTypeAliasesPackage());// &#125; return factory; &#125; /** * MapperLocations 转化为Resource[] * @param mapperLocations * @return */ public Resource[] resolveMapperLocations(String[] mapperLocations) &#123; ResourcePatternResolver resourceResolver = new PathMatchingResourcePatternResolver(); List&lt;Resource&gt; resources = new ArrayList&lt;&gt;(); if (mapperLocations != null) &#123; for (String mapperLocation : mapperLocations) &#123; try &#123; Resource[] mappers = resourceResolver.getResources(mapperLocation); resources.addAll(Arrays.asList(mappers)); &#125; catch (IOException ignore) &#123; // ignore ignore.printStackTrace(); &#125; &#125; &#125; return resources.toArray(new Resource[resources.size()]); &#125; &#125;&#125; 参考链接 mybatis-spring-boot-multi-ds-demo Multi Datasource","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"爬虫设计","slug":"爬虫设计","date":"2018-04-27T09:31:21.000Z","updated":"2021-05-06T09:53:24.060Z","comments":true,"path":"2018/04/27/爬虫设计/","link":"","permalink":"https://flyingglass.github.io/2018/04/27/爬虫设计/","excerpt":"","text":"一个设计良好的爬虫必须满足如下需求： 分布式：多机分布式执行 可伸缩性：爬虫结构能够通过额外的机器和带宽来提交爬虫速度 性能和有效性：爬虫系统必须有效使用各种系统资源，例如Cpu，内存和网络IO 更新：因为数据来源经常更新，爬虫应该取得已经获取数据的新的拷贝 可扩展性：为了能够支持新的数据格式和新的抓取协议，爬虫架构应该设计成模块化的形式 爬虫模块 URL Frontier(url队列) DNS模块 获取模块（使用http协议获取url代表的数据或页面） 解析模块提取api数据 重复消除模块决定一个解析出来的链接是否已经在URL Frontier或者最近下载过","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"nexus的npm配置","slug":"nexus的npm配置","date":"2018-04-26T07:05:33.000Z","updated":"2021-05-06T09:57:37.220Z","comments":true,"path":"2018/04/26/nexus的npm配置/","link":"","permalink":"https://flyingglass.github.io/2018/04/26/nexus的npm配置/","excerpt":"","text":"nexus是可以搭建maven，npm，docker，pypi私有仓库的工具，本文记录npm的安装配置 配置nexus的npm 添加认证：按图示进入realms设置页，将npm Bearer Token Realm加入active即可。 npm-taobao-proxy 123Remote storage:https://registry.npm.taobao.org/Use the Nexus truststore:勾选Blob store:npm npm-3rd-hosted 12Blob store:npmDeployment policy: Allow redeploy npm-group(注意Members顺序) 1234Blob store:npmMembers:- npm-3rd-hosted- npm-taobao-proxy 配置本地npm 用户级别配置 ~/.npmrc(若没有需要新建) .npmrc配置如下： 1registry = http://ip:port/nexus/repository/npm-group/ 参考链接 nexus搭建npm私服","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nexus的maven配置","slug":"nexus的maven配置","date":"2018-04-26T06:44:44.000Z","updated":"2021-05-06T09:57:32.948Z","comments":true,"path":"2018/04/26/nexus的maven配置/","link":"","permalink":"https://flyingglass.github.io/2018/04/26/nexus的maven配置/","excerpt":"","text":"nexus是可以搭建maven，npm，docker，pypi私有仓库的工具，本文记录maven的安装配置 配置nexus的maven maven-aliyun-proxy 12Remote storage:http://maven.aliyun.com/nexus/content/groups/publicBlob store:maven maven-cloudera-proxy 12Remote storage:https://repository.cloudera.com/artifactory/cloudera-repos/Blob store:maven maven-hosted-snapshots 123Version policy: SnapshotBlob store:mavenDeployment policy: Allow redeploy maven-hosted-releases 123Version policy: ReleaseBlob store:mavenDeployment policy: Allow redeploy maven-times-group(注意Members顺序) 123456789Blob store:mavenMembers:- maven-aliyun-proxy- maven-cloudera-proxy- maven-hosted-releases- maven-hosted-snapshots- maven-central- maven-releases- maven-snapshots 配置本地maven 全局配置 mac下brew的配置：/usr/local/Cellar/maven/3.5.2/libexec/conf/settings.xml linux下的配置：$MAVEN_HOME/conf/settings.xml 用户级别配置 ~/.m2/settings.xml(若没有需要新建) settings.xml配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;settings&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;!--This sends everything else to /public --&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://ip:port/nexus/repository/maven-times-group/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!--Enable snapshots for the built in central repo to direct --&gt; &lt;!--all requests to nexus via the mirror --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://central&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;central&lt;/id&gt; &lt;url&gt;http://central&lt;/url&gt; &lt;releases&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt; &lt;snapshots&gt;&lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;activeProfiles&gt; &lt;!--make the profile active all the time --&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt; &lt;/activeProfiles&gt; &lt;servers&gt; &lt;!--server username and password --&gt; &lt;server&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexus-snapshot&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;&lt;/settings&gt; 发布项目到nexus仓库配置 在项目的pom.xml如下配置： 12345678910111213&lt;!--nexus 设置--&gt;&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus-releases&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;http://ip:port/nexus/repository/maven-hosted-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus-snapshot&lt;/id&gt; &lt;name&gt;Nexus Snapshot Repository&lt;/name&gt; &lt;url&gt;http://ip:port/nexus/repository/maven-hosted-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt;","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"stellar指南","slug":"stellar指南","date":"2018-04-26T06:29:04.000Z","updated":"2021-04-29T09:45:31.279Z","comments":true,"path":"2018/04/26/stellar指南/","link":"","permalink":"https://flyingglass.github.io/2018/04/26/stellar指南/","excerpt":"","text":"Stellar是一个连接银行、支付系统以及广大民众的平台 LUMENLumen是Stellar网络的内置资产，一个lumen是一种类似数字货币单位，就像一个比特币一样，XLM是lumen的简称。2014年，Stellar网络发行了1000亿个stellar,这是该网络内置资产的原名。 为什么Stellar网络需要一种内置资产？Stellar网络提供了分布式数据库上共享公共账本的所有创新功能，分布式数据库通常指的是区块链技术。Stellar网络的内置货币lumen有两种用途： 首先，lumen起到了一定的过滤垃圾邮件的作用。每笔交易都会收取少部分的费用——0.00001lumen。该费用可防止恶意用户充斥网络（也称为）“DoS攻击”)。Lumen是一种安全的代币，可以缓和DOS攻击，这种攻击不仅会在账本中生成大量的交易，还会占据大量的空间。 同样，Stellar网络要求所有账户至少持有20个lumen的余额，因为这可以确保账户的真实性，从而让网络形成无缝的交易流。 其次，lumen可以促进多币种交易。有些情况下，lumen可以促进没有直接的大型市场的货币对之间的交易，充当桥梁的作用。当lumen和相关的每种货币之间存在流动性市场时，这种功能就不可或缺了。 我只有用lumen才能使用Stellar网络吗？Stellar网络可免费使用。其代码是开源的，拥有Apache许可证。 如果你想在在线网络上进行交易，就要用lumen来支付这些交易的费用。正是因为这种设计，Stellar上的交易成本很低。截止2016年，一个lumen可以支付10万比交易，目前一个lumen的价格大概CNY￥1.34。 参考链接 stellar stellar-cn 深扒恒星币（Stellar）","categories":[{"name":"区块链","slug":"区块链","permalink":"https://flyingglass.github.io/categories/区块链/"}],"tags":[]},{"title":"jenkins部署springboot应用","slug":"jenkins部署springboot应用","date":"2018-04-08T08:24:50.000Z","updated":"2021-05-06T09:57:11.026Z","comments":true,"path":"2018/04/08/jenkins部署springboot应用/","link":"","permalink":"https://flyingglass.github.io/2018/04/08/jenkins部署springboot应用/","excerpt":"","text":"最近在项目中使用持续交付(CI)工具jenkins部署spring-boot应用，碰到不少问题，特以此文记录 jenkins配置 插件配置 java,maven,git配置 部署脚本123456789101112131415161718192021222324252627#!/usr/bin/env bashBUILD_ID=dontKillMePROJ_NAME=\"backend-web-service\"LOG_PATH=\"/mnt/data01/logs\"MODULE_JAR_PATH=\"$&#123;WORKSPACE&#125;/backend-web-service/target/$&#123;PROJ_NAME&#125;.war\"###### Stop servicepid=`ps -ef | grep $PROJ_NAME | grep -v grep | awk '&#123;print $2&#125;'`if [ -n \"$pid\" ]then sudo kill -9 $pidfi ###### Maven build packagecd $&#123;WORKSPACE&#125; &amp;&amp; mvn clean package -Dmaven.test.skip=true###### Start service# Check Log Pathif [ ! -d $&#123;LOG_PATH&#125; ]; then sudo mkdir -p $&#123;LOG_PATH&#125;fisudo nohup java -jar $&#123;MODULE_JAR_PATH&#125; --spring.profiles.active=dev --logback.logdir=$&#123;LOG_PATH&#125; 1&gt;/dev/null 2&gt;/dev/null &amp;echo \"Log Path : $&#123;LOG_PATH&#125;\" 碰到的问题 由于jenkins构建完毕后会杀掉所有启动的进程，可以通过改变BUILD_ID的值来防止后台进程被杀死 如果采用maven工程进行构建，修改BUILD_ID无效，后台进程随着jenkins构建完成后仍被杀死","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"jenkins安装","slug":"jenkins安装","date":"2018-04-08T07:32:01.000Z","updated":"2021-05-06T09:57:08.383Z","comments":true,"path":"2018/04/08/jenkins安装/","link":"","permalink":"https://flyingglass.github.io/2018/04/08/jenkins安装/","excerpt":"","text":"最近spring-boot项目用到了持续集成（CI）工具Jenkins进行部署，使用nginx作为反向代理 jenkins的安装123# ubuntu 16.04 LTS 安装apt-get updateapt-get install -y jenkins 使用nginx配置jenkins nginx反向代理配置 12345678# jenkinslocation ^~ /jenkins &#123; proxy_pass http://127.0.0.1:8080/jenkins; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme;&#125; /etc/default/jenkins添加--prefix=$PREFIX配置 1JENKINS_ARGS=\"--webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT --prefix=$PREFIX\" 启动jenkins 1service jenkins start 参考链接 jenkins download how-to-configure-jenkins-with-ssl-using-an-nginx-reverse-proxy","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nexus本地仓库搭建","slug":"nexus本地仓库搭建","date":"2018-04-08T03:27:23.000Z","updated":"2021-05-06T09:57:26.510Z","comments":true,"path":"2018/04/08/nexus本地仓库搭建/","link":"","permalink":"https://flyingglass.github.io/2018/04/08/nexus本地仓库搭建/","excerpt":"","text":"nexus是可以搭建maven，npm，docker，pypi私有仓库的工具，本文记录本地nexus的安装配置 docker-compose安装nexus 其中docker-compose.yml的配置： 1234567891011version: '2' # nexus nexus: image: sonatype/nexus3:3.2.0 restart: always ports: - \"8081:8081\" volumes: - /srv/docker/nexus/nexus-data:/nexus-data:Z environment: - NEXUS_CONTEXT=nexus 配置挂载目录的权限： 1mkdir -p /srv/docker/nexus/nexus-data &amp;&amp; chown -R 200 /srv/docker/nexus/nexus-data 配置nexus的仓库组nexus默认的仓库类型有以下3种： group：组仓库，用于方便开发人员自己设定的仓库 hosted：宿主仓库，内部项目的开发仓库 proxy：代理仓库，从远程中央仓库中寻找数据的仓库 示例代码-github参考链接 使用nexus3搭建私有仓库 Sonatype Nexus3 Docker: sonatype/nexus3 maven私服nexus3.x环境配置","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"truffle开发入门","slug":"truffle开发入门","date":"2018-03-19T12:13:51.000Z","updated":"2021-04-29T09:45:31.280Z","comments":true,"path":"2018/03/19/truffle开发入门/","link":"","permalink":"https://flyingglass.github.io/2018/03/19/truffle开发入门/","excerpt":"","text":"Truffle是以太坊最受欢迎的开发框架，在搭建第一个区块链程序的过程，由于truffle的版本更新为3.0以上，按照大部分部署教程无法运行，特以此记录。本文部署环境为macosx，采用homebrew套件进行部署，homebrew的安装可参考之前文章homebrew安装 Node环境配置1234# Install nodebrew install node# Test node -v Truffle安装123456# Install truffle v3.2.1npm install -g truffle@3.2.1# Install testrpcnpm install -g ethereumjs-testrpc# Testrpctestrpc Webpack Truffle Box12345678# webpackmkdir webpack &amp;&amp; cd webpack # Truffle inittruffle ubox webpack# Compile and migratetruffle compile &amp;&amp; truffle migrate# Servenpm run dev 参考链接 how-to-install-nodejs-and-npm-on-mac-using-homebrew file-node-npm-install-md truffle-init-webpack","categories":[{"name":"区块链","slug":"区块链","permalink":"https://flyingglass.github.io/categories/区块链/"}],"tags":[]},{"title":"区块链权威指南","slug":"区块链权威指南","date":"2018-03-19T08:20:48.000Z","updated":"2021-04-29T09:45:31.281Z","comments":true,"path":"2018/03/19/区块链权威指南/","link":"","permalink":"https://flyingglass.github.io/2018/03/19/区块链权威指南/","excerpt":"","text":"区块链架构剖析区块链权威指南读书笔记 区块链1.0架构：比特币区块链 比特币前端 钱包：非决定性钱包，决定性钱包 HTTP/JSON RPC API 命令行工具bitcoin-cli 比特币浏览器bx 图形开发工具（Qt） 比特币后端 区块管理 区块验证 内存池管理 邻接点管理 共识管理 规则管理 密码模块 签名模块 脚本引擎 挖矿 HTTP/JSON RPC服务端 Berkeley DB和LevelDB数据库 P2P网络管理 ZMQ队列管理 区块链2.0架构：以太坊区块链 EVM高级语言 ：Solidity(Js)、Serpent(Python)、LLL(Lisp)、Mutan(C已废弃) 主流脚手架：Go，Solidity(js), Truffle","categories":[{"name":"区块链","slug":"区块链","permalink":"https://flyingglass.github.io/categories/区块链/"}],"tags":[]},{"title":"cdh下Sqoop的使用","slug":"cdh下Sqoop的使用","date":"2018-03-13T07:17:36.000Z","updated":"2021-04-29T09:45:31.259Z","comments":true,"path":"2018/03/13/cdh下Sqoop的使用/","link":"","permalink":"https://flyingglass.github.io/2018/03/13/cdh下Sqoop的使用/","excerpt":"","text":"最近项目需要导出Hdfs的数据到mysql中，采用sqoop作为主要的导出方案，在使用sqoop2和sqoop1的过程中，碰到一些问题以及对应的一些解决方案。思绪来的快，去的也快，特此记录 项目中采用cdh搭建的hadoop大数据平台，其中cdh的版本为5.12.0，由于涉及到cdh中sqoop2的部署，使用sqoop2的Java api，sqoop2的源码修改以支持自定义分隔符以及如何采用sqoop1的Java api解决方案，故通过分小节来记录，主要小节分类如下： Sqoop2的解决方案： cdh下Sqoop2的部署 Sqoop2 Java api解析 Sqoop2 自定义分隔符 Sqoop1的解决方案： cdh下Sqoop1的部署 Sqoop1 Java api解析 Sqoop1 remote ssh执行命令","categories":[{"name":"大数据","slug":"大数据","permalink":"https://flyingglass.github.io/categories/大数据/"}],"tags":[]},{"title":"cdh下Sqoop2的安装","slug":"cdh下Sqoop2的安装","date":"2018-03-13T06:51:03.000Z","updated":"2021-04-29T09:45:31.259Z","comments":true,"path":"2018/03/13/cdh下Sqoop2的安装/","link":"","permalink":"https://flyingglass.github.io/2018/03/13/cdh下Sqoop2的安装/","excerpt":"","text":"本文中采用的cdh版本为5.12.0，如何部署cdh可以参考之前的博文cdh安装准备和cdh5.12.0的安装 sqoop2配置 cdh安装sqoop2 配置Drivers 配置hadoop依赖 cdh安装sqoop2cdh5.12.0下安装sqoop2，参见官方的安装手册，直接在搭建好的cdh管理界面添加Sqoop2服务组件，然后一路选择继续安装，等待Sqoop2服务启动完毕。 配置Drivers由于本文是采用hdfs导出数据到mysql，故需配置mysql的connector的jar依赖包，参考官方的指南，下载mysql-connector-java.jar包，然后拷贝到对应的cdh的默认sqoop2的目录/var/lin/sqoop2目录中，最终的目录结构如下： 12[~]ls /var/lib/sqoop2mysql-connector-java.jar postgresql-9.0-801.jdbc4.jar repository tomcat-deployment 配置hadoop超链接123cd /opt/cloudera/parcels/CDH/lib/hadoop/clientsudo ln -s ../../hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar .sudo ln -s ../../hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar . Tips：配置完毕后在cdh管理界面重启Sqoop2组件服务，否则配置不生效 参考链接 安装文档 JDBC Drviers for Sqoop Sqoop Error: Could not start job","categories":[{"name":"大数据","slug":"大数据","permalink":"https://flyingglass.github.io/categories/大数据/"}],"tags":[]},{"title":"Resource和Autowired注解区别","slug":"Resource和Autowired注解区别","date":"2018-03-13T06:29:18.000Z","updated":"2021-05-06T09:43:45.075Z","comments":true,"path":"2018/03/13/Resource和Autowired注解区别/","link":"","permalink":"https://flyingglass.github.io/2018/03/13/Resource和Autowired注解区别/","excerpt":"","text":"最近在使用Mybatis处理通用Mapper封装时，使用@Resource进行注入bean时，遇到一些问题，本文特此记录： @Autowired默认按照类型进行装配，要求依赖对象必须存在，如果允许为null，可以设置required属性为false；可以结合@Qualifier注解使用名称装配。 @Resource主要有两种属性type和name。如果使用name属性，则使用byName进行注入；如果使用type属性，则使用byType进行注入；如果注入策略既不制定name也不指定type属性，将通过反射机制使用byName进行注入。其中@Resource的装配顺序如下： 指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常 指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常 指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常 既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配 在泛型装配时，@Resouce和@Autowired的区别： 12345678/*** 针对@Resouce会出现无法装配的情形* @Autowired按照类型装配，在装配泛型是按照T的真实类型装配*/public abstract class AbstractService&lt;T&gt; implements IService&lt;T&gt; &#123; @Autowired protected IMapper&lt;T&gt; IMapper;&#125; 参考链接 Spring注解@Autowired和@Resource区别 Spring @Autowired+@Qualifier与@Resource的区别","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"hexo布局","slug":"hexo布局","date":"2018-03-02T09:00:55.000Z","updated":"2021-05-06T09:54:18.743Z","comments":true,"path":"2018/03/02/hexo布局/","link":"","permalink":"https://flyingglass.github.io/2018/03/02/hexo布局/","excerpt":"","text":"hexo布局Hexo有三种默认布局：post、page和draft，他们分别对应不同的目录，而自定义的其他布局和post相同，都将存储到source/_posts目录。 布局 目录 post source/_posts page source draft source/_drafts 草稿hexo的特殊布局：draft，这种布局建立时会被保存到source/_drafts文件夹，可通过publish命令将草稿移动到source/_posts目录下，该命令的使用方式与new类似，也可以通过layout指定布局。 1$hexo publish [layout] &lt;title&gt; 草稿默认不会显示在页面中，可在执行时加上--draft参数，或是把render_drafts参数设置为true来预览草稿。 参考链接 hexo写作","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nginx代理tomcat导致css或js加载失败","slug":"nginx代理tomcat导致css或js加载失败","date":"2018-02-28T02:33:58.000Z","updated":"2021-05-06T09:57:46.344Z","comments":true,"path":"2018/02/28/nginx代理tomcat导致css或js加载失败/","link":"","permalink":"https://flyingglass.github.io/2018/02/28/nginx代理tomcat导致css或js加载失败/","excerpt":"","text":"Web服务器经常会使用nginx作前置路由，在使用nginx配置Web服务器负载均衡、动静分离，会碰到设置反向代理后导致前端资源无法加载的问题 nginx反向代理的tomcat服务器导致前端资源css或js加载失败大概可以分为：端口丢失、真实ip或端口获取错误、js或者css太大加载失败等情形 端口丢失 之前笔者也有文章单独介绍，可参考nginx转发丢失端口的问题 反向代理获取真实ip（域名）、端口、协议nginx反向代理后，servlet应用通过request.getRemoteAddr()取到的IP是nginx的IP，并非客户端的真实IP；通过request.getRequestUrl()获取的ip（域名）、端口、协议都是nginx对应的参数。 比如nginx的配置： 12345678910111213141516171819http &#123; upstream backend &#123; server 127.0.0.1:8080; #server backend1.example.com wight=5; #server 127.0.0.1:8080 max_fails=3 fail_timeout=30s; #server backup1.example.com backup; &#125; server &#123; listen 80; server_name your.domain.com; location /test &#123; proxy_pass http://backend/test; &#125; ... &#125;&#125; 在浏览器打开http://your.domain.com/test访问servlet应用，获取客户端IP和URL： 123log.info(\"RemoteAddr:&#123;&#125;, URL:&#123;&#125;\", request.getRemoteAddr(), request.getRequestURL());//输出结果RemoteAddr:127.0.0.1, URL:http://127.0.0.1:8080/test 针对tomcat+nginx的解决方案： nginx添加如下配置,： 1234proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header X-Forwarded-Proto $scheme; 如果不知道如何添加，可参考nginx转发丢失端口的问题。添加完后，输出结果如下： 12//输出结果RemoteAddr:127.0.0.1, URL:http://浏览器的ip地址/test 发现RemoteAddr仍然获取不正确，解决方案如下： 方案一：通过request.getHeader(&quot;X-Forwrad-For&quot;)或request.getHeader(&quot;X-Real-IP&quot;)获取到nginx配置的Header。 方案二： 配置tomcat，通过Servlet API的request.getRemoteAddr()方法获取客户端的IP。Tomcat的server.xml，在Host元素内最后加入： 1&lt;Valve className=\"org.apache.catalina.valves.RemoteIpValve\" /&gt; JS或css无法完全加载nginx的代理缓存区，默认较小导致部分文件出现加载不全的问题，比较典型的如jQuery框架，可以通过配置调整nginx的缓存区即可。 最终完整配置如下： 1234567891011121314151617181920212223242526http &#123; # http_proxy proxy_buffer_size 128k; proxy_buffers 32 128k; proxy_busy_buffers_size 128k; upstream backend &#123; server 127.0.0.1:8080; &#125; server &#123; listen 80; server_name your.domain.com; location /test &#123; proxy_pass http://backend/test; # proxy_params proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; &#125; ... &#125; ...&#125; 关于nginx的http_proxy模块参数含义： 参考链接： nginx tomcat负载均衡、动静分离 Nginx+Tomcat代理环境下JS无法完全加载问题的处理 Jetty/Tomcat + Nginx反向代理获取客户端真实IP、域名、协议、端口 nginx基本使用系列(三)_nginx常用配置文件解析","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"ag安装","slug":"ag","date":"2018-02-26T11:16:21.000Z","updated":"2021-05-06T09:47:38.997Z","comments":true,"path":"2018/02/26/ag/","link":"","permalink":"https://flyingglass.github.io/2018/02/26/ag/","excerpt":"","text":"Installing ag on CentOS Prerequistes libpcre liblzma Download, build and install12345678910sudo yum install -y pcre-develsudo yum install xz-develcd /usr/local/src &amp;&amp; sudo git clone https://github.com/ggreer/the_silver_searcher.gitcd the_silver_searchersudo ./build.shsudo make installwhich ag","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"web常见漏洞","slug":"web常见漏洞","date":"2018-02-26T07:13:29.000Z","updated":"2021-05-06T09:51:16.659Z","comments":true,"path":"2018/02/26/web常见漏洞/","link":"","permalink":"https://flyingglass.github.io/2018/02/26/web常见漏洞/","excerpt":"","text":"项目采用springboot作为web框架，涉及到常见的漏洞，特此记录 启用了不安全的HTTP 方法 描述：启用了不安全的 HTTP 方法， 可能会在 Web 服务器上上载、修改或删除 Web 页面、脚本和文件 原因：Web 服务器或应用程序服务器是以不安全的方式配置的 解决方案： 方案一： 如果采用springboot内嵌的tomcat容器，可以在application.yml或application.properties增加如下配置： 12#解决不安全的HTTP方法漏洞 server.tomcat.port-header=HEAD,PUT,DELETE,OPTIONS,TRACE,COPY,SEARCH,PROPFIND 方案二： 代码增加对tomcat的配置，代码如下： 1234567891011121314151617181920212223242526@Configuration public class TomcatConfig &#123; @Bean public EmbeddedServletContainerFactory servletContainer() &#123; TomcatEmbeddedServletContainerFactory tomcat = new TomcatEmbeddedServletContainerFactory() &#123;// 1 protected void postProcessContext(Context context) &#123; SecurityConstraint securityConstraint = new SecurityConstraint(); securityConstraint.setUserConstraint(\"CONFIDENTIAL\"); SecurityCollection collection = new SecurityCollection(); collection.addPattern(\"/*\"); collection.addMethod(\"HEAD\"); collection.addMethod(\"PUT\"); collection.addMethod(\"DELETE\"); collection.addMethod(\"OPTIONS\"); collection.addMethod(\"TRACE\"); collection.addMethod(\"COPY\"); collection.addMethod(\"SEARCH\"); collection.addMethod(\"PROPFIND\"); securityConstraint.addCollection(collection); context.addConstraint(securityConstraint); &#125; &#125;; return tomcat; &#125; &#125; 测试： 123456789#使用curl测试：curl -v -X OPTIONS http://www.example.com/test/查看响应的 Allow: GET, HEAD, POST, PUT, DELETE, OPTIONS# 看是否能上载来判断攻击是否生效。curl -v -T test.html http://www.example.com/test/test.html# 找一个存在的页面，如test2.html，如果删除成功，则攻击有效。curl -X DELETE http://www.example.com/test/test2.html 参考链接： 验证启用了不安全的HTTP方法 spring boot使用内嵌的tomcat解决不安全的HTTP方法安全漏洞 Apache JServ protocol service 描述：Apache JServ协议(AJP)是一种二进制协议，可以从web服务器代理入站请求。位于web服务器后的应用服务器，不建议将AJP服务公开访问。如果AJP被错误配置，它可能允许攻击者访问内部资源。 解决方案： 找到tomcat的配置文件目录，路径如下${tomcat目录}/conf/server.xml 注释如下行 &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 参考链接： stackoverflow禁用AJP服务 The Apache Tomcat Connector Host header attack 描述：在许多情况下，开发人员信任HTTP主机头值并使用它来生成链接、导入脚本，甚至生成密码重置与它的值的链接。这是一个非常糟糕的想法，因为HTTP主机头可以由攻击者控制，可以利用网络缓存中毒和滥用替代渠道的密码重置邮件。 解决方案： 使用getServerName()代替getHeader(“host”); 在Apache和nginx通过设置一个虚拟机来记录所有的非法Host header，或者在Apache和nginx里指定一个ServerName名单; 同时，Apache开启UseCanonicalName选项 1234# Apache在extra下的httpd-default.conf文件或者虚拟主机加入如下配置UseCanonicalName On# Nginx配置server_name example.com; 参考链接： 被检测出有HTTP HOST头部攻击 host头攻击解决方法 Slow HTTP Denial of Service Attack 描述：web服务器容易受到HTTP DoS(拒绝服务)攻击的影响，HTTP POST DoS攻击依赖于HTTP协议的设计要求。在处理之前完全接收到服务器，如果HTTP请求没有完成，或者传输速率非常低，服务器保持它的资源忙着等待其余的数据。如果服务器保存了太多的资源，这会造成拒绝服务。 解决方案： 配置防火墙 iptables -A INPUT -p tcp --syn --dport 端口号 -m connlimit --connlimit-above 50 -j REJECT 参考链接 how-to-protect-tomcat-7-against-slowloris-attack ​","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"nginx转发丢失端口问题","slug":"nginx转发丢失端口问题","date":"2018-02-24T09:57:56.000Z","updated":"2021-05-06T09:58:02.263Z","comments":true,"path":"2018/02/24/nginx转发丢失端口问题/","link":"","permalink":"https://flyingglass.github.io/2018/02/24/nginx转发丢失端口问题/","excerpt":"","text":"nginx对于redirect location的处理，会造成端口丢失的现象。针对nginx处理非80标准端口进行redirect时导致端口丢失的问题，本文主要介绍nginx的反向代理和访问目录缺失/这两种情形 以下为两种情形的参考范例： 123456789101112131415# 反向代理listen 26479 default_server;location / &#123; root /var/www/html; proxy_pass http://127.0.0.1:8080/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;&#125;# 访问/gitlab的时候会自动加上/成为/gitlab/listen 26479 default_server;location /gitlab &#123; root /var/www/html;&#125; 反向代理查看nginx官方文档提及的说明： 123An unchanged “Host” request header field can be passed like this:proxy_set_header Host $http_host; 同时参考gitlab-ce使用非标准端口访问的方案，查看gitlab-ce的nginx配置如下： proxy_set_header Host $http_host; 通过查看nginx软件包，发现nginx已提供参考文件/etc/nginx/proxy_params 1234proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header X-Forwarded-Proto $scheme; 由于我使用的centos下面手动编译的nginx并未找到proxy_params，我的nginx位于/usr/local/nginx，于是通过vim /usr/local/nginx/conf/proxy_params输入以上内容，配置nginx的location如下： 1234location /gitlab &#123; proxy_pass http://127.0.0.1:9090/gitlab; include proxy_params;&#125; 也可以手工输入proxy_params的内容到location配置段下 访问目录缺失/比如$document_root存在data/index.html文件，但是访问的时候最后没加/，nginx会自动给你带上/，返回一个301重定向(这个行为和apache一致)，但是当nginx监听的是非标准端口，这个301返回的Location没有端口号，导致浏览器请求出错。用curl可以很明显的看到这一点: 123456789101112131415161718192021222324252627[~]# curl -v 127.0.0.1:9090/data* About to connect() to 127.0.0.1 port 9090 (#0)* Trying 127.0.0.1...* Connected to 127.0.0.1 (127.0.0.1) port 9090 (#0)&gt; GET /data HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: 127.0.0.1:9090&gt; Accept: */*&gt; &lt; HTTP/1.1 301 Moved Permanently* Server nginx is not blacklisted&lt; Server: nginx&lt; Date: Mon, 26 Feb 2018 02:20:49 GMT&lt; Content-Type: text/html; charset=utf-8&lt; Content-Length: 178&lt; Connection: keep-alive&lt; Cache-Control: no-cache&lt; Location: http://127.0.0.1/data/&lt;&lt;html&gt;&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;* Connection #0 to host 127.0.0.1 left intact 可以看到Location端口号丢失了，这个和反向代理不一样。通过google搜索，在stackoverflow的question找到了解决方案： 123if (-d $request_filename) &#123; rewrite [^/]$ $scheme://$http_host$uri/ permanent;&#125; 通过对URL进行重写带上端口","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"cdh5.12.0安装","slug":"cdh5-12-0安装","date":"2018-02-04T06:00:09.000Z","updated":"2021-04-29T09:45:31.258Z","comments":true,"path":"2018/02/04/cdh5-12-0安装/","link":"","permalink":"https://flyingglass.github.io/2018/02/04/cdh5-12-0安装/","excerpt":"","text":"安装Cloudera Manager本文采用离线安装cdh5.12.0，所有安装包的版本均为5.12.0，安装所需的离线包均需提前下载，下文将给出对应下载链接地址和配置。安装的操作系统为centos 7及以上系统，其他系统和版本可以需自行下载对应的rpm 下载cloudera-manager.repo（所有节点均需配置） 下载链接 cloudera-manage.repo配置(所有节点均需配置，仅仅展示主节点的配置如下) 1234567891011[root@n171 cdh] cat /etc/yum.repos.d/cloudera-manager.repo[cloudera-manager]name = Cloudera Manager, Version 5.12.0baseurl = https://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.12.0/gpgkey = https://archive.cloudera.com/redhat/cdh/RPM-GPG-KEY-clouderagpgcheck = 1# 验证生效yum clean all yum list | grep cloudera # 确认显示的版本是否正确（我这里显示5.12.0-1.cm5120.p0.120.el7） 下载cloudera-manager-installer.bin（主节点需配置，从节点无须配置） 下载链接 放置在/opt目录，并添加执行权限 1chmod +x /opt/cloudera-manager-installer.bin 配置完毕后验证[root@n171 cdh] ll /opt/cloudera-manager-installer.bin -rwxr-xr-x 1 root root 519670 6月 21 02:43 /opt/cloudera-manager-installer.bin 下载RPMS 下载链接(下载目录下所有文件) 新建放置目录 mkdir -p /opt/cloudera-RPMS/ 下载完毕后： master节点，目录结构如下 1234567[root@n171 cdh]# ls /opt/cloudera-RPMS/cloudera-manager-agent-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmcloudera-manager-daemons-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmcloudera-manager-server-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmcloudera-manager-server-db-2-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmenterprise-debuginfo-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmoracle-j2sdk1.7-1.7.0+update67-1.x86_64.rpm slave节点(不需要server相关的rpm，只需要agent相关rpm)，目录结构如下123[root@n172 parcel-repo]# ls /opt/cloudera-RPMS/cloudera-manager-agent-5.12.0-1.cm5120.p0.120.el7.x86_64.rpmcloudera-manager-daemons-5.12.0-1.cm5120.p0.120.el7.x86_64.rpm 下载CDH-Parcel 下载链接 新建放置目录 mkdir -p /opt/cloudera/parcel-repo/ 下载完毕后(所有节点)，目录结构如下 12345[root@n171 cdh]# ls /opt/cloudera/parcel-repo/CDH-5.12.0-1.cdh5.12.0.p0.29-el7.parcelCDH-5.12.0-1.cdh5.12.0.p0.29-el7.parcel.shamanifest.json注意:下载后将.sha1文件后缀更改为.sha 执行安装 所有节点执行 12cd /opt/cloudera-RPMS yum -y localinstall --nogpgcheck *.rpm master节点执行 1234# 拷贝cloudera-manage-installer.bin到/opt/# +x 权限sudo chmod +x /opt/cloudera-manager-installer.bin/opt/cloudera-manager-installer.bin 访问安装界面12http://Master主机的IP:7180/ 如果不能访问，稍等刷新再试，服务启动需要一定时间 参考链接： CDH5.12.0安装官网链接","categories":[{"name":"大数据","slug":"大数据","permalink":"https://flyingglass.github.io/categories/大数据/"}],"tags":[]},{"title":"cdh5.12.0安装准备","slug":"cdh安装准备","date":"2018-02-04T04:05:09.000Z","updated":"2021-04-29T09:45:31.259Z","comments":true,"path":"2018/02/04/cdh安装准备/","link":"","permalink":"https://flyingglass.github.io/2018/02/04/cdh安装准备/","excerpt":"","text":"简介CDH是cloudera公司开发的一个快速部署、高效管理Hadoop和其各种组件的一个商业化产品。 主要分为两部分，分别为Cloudera Manager和CDH软件包。其中Cloudera Manager负责集群的部署与管理。CDH软件包囊括了hdaoop各类的组件的安装包，例如hive、hdfs、spark等等。 系统要求摘自官网文档:The four hosts in the cluster must satisfy the following requirements: The hosts must have at least 10 GB RAM. You must have root or password-less sudo access to the hosts. If using root, the hosts must accept the same root password. The hosts must have Internet access to allow the wizard to install software from archive.cloudera.com Run a supported OS: See CDH 5 and Cloudera Manager 5 Requirements and Supported Versions. SLES - SUSE Linux Enterprise Server 11, 64-bit. Service Pack 2 or higher is required. The Updates repository must be active and SUSE Linux Enterprise Software Development Kit 11 SP1 is required. Debian - Wheezy (7.0 and 7.1), 64-bit. Ubuntu - Trusty (14.04) and (Precise) 12.04, 64-bit.If your environment does not satisfy these requirements, the procedure described in this guide might not work. For information about other Cloudera Manager installation options and requirements,see Installing Cloudera Manager and CDH. HADOOP版本目前Hadoop比较流行的主要有2个版本，Apache和Cloudera版本。 Apache Hadoop：维护人员比较多，更新频率比较快，但是稳定性比较差。 Cloudera Hadoop（CDH）：CDH：Cloudera公司的发行版本，基于ApacheHadoop的二次开发，优化了组件兼容和交互接口、简化安装配置、增加Cloudera兼容特性。 CDH安装准备（本文操作系统为centos7及以上系统）jdk 配置 vim /etc/profile12345678910111213# JAVAJAVA_HOME=/usr/local/jdk1.8.0_151JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib# PATHPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin# exportexport PATH JAVA_HOME JRE_HOME CLASS_PATH# 环境变量生效 . /etc/profile hosts修改(每一个节点都需要修改)123456789[root@n171 cloudera-RPMS] cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.30.171 n171192.168.30.172 n172192.168.30.173 n173192.168.30.174 n174 hostname配置（所有节点都需要修改需要重启）12[root@n171] cat /etc/hostnamen171 防火墙配置(cdh端口过多先关闭防火墙)12345# 关闭防火墙sudo systemctl stop firewalld.service# 关闭开机启动sudo systemctl disable firewalld.service selinux123456789# 查看SELinux状态getenforce # 如果不是Disabled,修改 SELinux=disabled ，需要重启机器# 修改配置vim /etc/selinux/config# 查看配置内容 grep -v \"#\" /etc/selinux/config | grep -v '^$'SELINUX=disabledSELINUXTYPE=targeted ntp配置(分布式系统时间同步工具)参考linux鸟哥私房菜ntp master(本文以n171为主节点) 12345678910111213# 查看主节点n171的配置[root@n171 ~]grep -v \"#\" /etc/ntp.conf | grep -v \"^$\"# /etc/ntp.conf 配置driftfile /var/lib/ntp/driftrestrict default nomodify notrap nopeer noqueryrestrict 127.0.0.1restrict ::1restrict 192.168.30.0 mask 255.255.255.0 nomodify notrapserver ntp1.aliyun.com preferserver time1.aliyun.comincludefile /etc/ntp/crypto/pwkeys /etc/ntp/keysdisable monitor slave(所有从节点ntp客户端配置) 123456789101112# 查看从节点配置[root@n172 ~]grep -v \"#\" /etc/ntp.conf | grep -v \"^$\"# /etc/ntp.conf配置driftfile /var/lib/ntp/driftrestrict default nomodify notrap nopeer noqueryrestrict 127.0.0.1restrict ::1restrict namenode01server namenode01includefile /etc/ntp/crypto/pwkeys /etc/ntp/keysdisable monitor 参考链接： CDH5.12.0安装官网链接","categories":[{"name":"大数据","slug":"大数据","permalink":"https://flyingglass.github.io/categories/大数据/"}],"tags":[]},{"title":"minidlna配置","slug":"minidlna","date":"2018-01-13T13:48:50.000Z","updated":"2021-05-06T09:54:52.027Z","comments":true,"path":"2018/01/13/minidlna/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/minidlna/","excerpt":"","text":"ubuntu配置minidlna 删除minidlna依赖： 123sudo apt-get purge minidlnasudo apt-get remove minidlna sudo apt-get autoremove 安装： sudo apt-get install minidlna 修改配置 12345678910sudo vim /etc/default/minidlna# User and group the daemon should run asUSER=\"root\"#GROUP=\"minidlna\"sudo vim /etc/minidlna.conf# Specify the user name or uid to run as.user=rootsudo service minidlna restart 参考链接 Raspberry Pi （树莓派）折腾记之二 启动dlna权限问题","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"homebrew配置","slug":"homebrew","date":"2018-01-13T13:41:35.000Z","updated":"2021-05-06T09:48:21.461Z","comments":true,"path":"2018/01/13/homebrew/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/homebrew/","excerpt":"","text":"安装参考链接 1/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" homebrew替换为清华的源 替换formula 索引的镜像（即 brew update 时所更新内容） 1234567cd \"$(brew --repo)\"git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.gitcd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\"git remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.gitbrew update 替换Homebrew 二进制预编译包的镜像 12echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles' &gt;&gt; ~/.zshrcsource ~/.zshrc homebrew清华大学开源使用帮助","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"gogs配置","slug":"gogs","date":"2018-01-13T13:26:30.000Z","updated":"2021-05-06T09:55:58.509Z","comments":true,"path":"2018/01/13/gogs/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/gogs/","excerpt":"","text":"什么是 Gogs?Gogs 是一款极易搭建的自助 Git 服务。 Gogs目的Gogs 的目标是打造一个最简单、最快速和最轻松的方式搭建自助 Git 服务。使用 Go 语言开发使得 Gogs 能够通过独立的二进制分发，并且支持 Go 语言支持的 所有平台，包括 Linux、Mac OS X、Windows 以及 ARM 平台。 gogs配置gogs安装配置文档 gogs配置mysql数据库: gogs要求Mysql版本为5.7以上,配置如下: 升级MySQL到5.7 树莓派中安装MySQL 5.7 Install Mysql 5.7 on respbian jesse -pi 3 配置innodb和字符集为utf8mb4 123456789[client]default-character-set=utf8mb4[mysqld]character-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect=&apos;SET NAMES utf8mb4&apos;skip-character-set-client-handshake = true[mysql]default-character-set = utf8mb4","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"redis配置","slug":"redis","date":"2018-01-13T13:22:09.000Z","updated":"2021-05-06T09:50:58.918Z","comments":true,"path":"2018/01/13/redis/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/redis/","excerpt":"","text":"Prerequistes Centos 1yum -y install tcl Ubuntu 1apt-get -y install build-essential tcl redis源码编译安装12345678910111213141516REDIS_VERSION=4.0.1wget http://download.redis.io/releases/redis-$REDIS_VERSION.tar.gztar -zxvf redis-$REDIS_VERSION.tar.gzcd redis-$REDIS_VERSIONmakemake testsudo make install# Once the program has been installed# Redis comes with a built in script that sets up Redis to run as a background daemon.cd utilssudo ./install_server.sh# Start and stopsudo service redis_6379 startsudo service redis_6379 stop# To set Redis to automatically start at boot, run:sudo update-rc.d redis_6379 defaults /etc/redis/6379.conf 配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859➜ ~ grep -v \"#\" /etc/redis/6379.conf | grep -v \"^$\"bind 0.0.0.0protected-mode yesport 6379tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised nopidfile /var/run/redis_6379.pidloglevel noticelogfile /var/log/redis_6379.logdatabases 16always-show-logo yessave 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename dump.rdbdir /var/lib/redis/6379slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush noappendonly noappendfilename \"appendonly.aof\"appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yesaof-use-rdb-preamble nolua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events \"\"hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yes 设置redis密码12345678910# 搜索requirepass 即可查看密码，并且设置密码vim /etc/redis/redis.conf # 如果之前没有设置过密码可以通过密码命令行设置密码CONFIG get requirepass;# 设置密码 127.0.0.1:6379&gt; CONFIG set requirepass \"tutorialspoint\" OK 127.0.0.1:6379&gt; CONFIG get requirepass 1) \"requirepass\" 2) \"tutorialspoint","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]},{"title":"nginx配置","slug":"nginx","date":"2018-01-13T13:15:29.000Z","updated":"2021-05-06T09:57:42.766Z","comments":true,"path":"2018/01/13/nginx/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/nginx/","excerpt":"","text":"centos 6.5配置 /etc/yum.repos.d/目录下创建一个源配置文件nginx.repo： cd /etc/yum.repos.d/ vim nginx.repo 填写如下内容： [nginx] name=nginx repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=0 enabled=1 保存，则会产生一个/etc/yum.repos.d/nginx.repo文件。 直接执行如下指令即可自动安装好Nginx： yum install nginx -y 安装完成，下面直接就可以启动Nginx了： /etc/init.d/nginx start 现在Nginx已经启动了，直接访问服务器就能看到Nginx欢迎页面了的。 如果还无法访问，则需配置一下Linux防火墙。 iptables -I INPUT 5 -i eth0 -p tcp —dport 80 -m state —state NEW,ESTABLISHED -j ACCEPT service iptables save service iptables restart Nginx的命令以及配置文件位置： /etc/init.d/nginx start # 启动Nginx服务 /etc/init.d/nginx stop # 停止Nginx服务 /etc/nginx/nginx.conf # Nginx配置文件位置 chkconfig nginx on #设为开机启动 至此，Nginx已经全部配置安装完成。 nginx基础配置 123456789101112131415161718192021222324252627一台主机上适应多个服务器：在你的nginx通过代理的方式转发请求：配置如下vi /etc/nginx/nginx.conf在http加入下面的内容，参考：http://wiki.nginx.org/FullExamplehttp &#123;.... server &#123; listen 80; server_name www.a.com; charset utf-8; access_log /home/a.com.access.log main; location / &#123; proxy_pass http://127.0.0.1:80; &#125; &#125; server &#123; listen 80; server_name www.b.com; charset utf-8; access_log /home/b.com.access.log main; location / &#123; proxy_pass http://127.0.0.1:81; &#125; &#125;...&#125; centos 7.0 配置1234567891011121314NGINX_VERSION=1.10.1yum -y install gcc gcc-c++ make libtool zlib zlib-devel openssl openssl-devel pcre pcre-develwget http://nginx.org/download/nginx-$NGINX_VERSION.tar.gztar -zxvf nginx-$NGINX_VERSION.tar.gzcd nginx-$NGINX_VERSION./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-pcremake &amp;&amp; make install#sudo /usr/local/nginx/sbin/nginx ubuntu 14.04 配置123456789101112NGINX_VERSION=1.10.1apt-get -y install libpcre3-dev aptitude libssl-dev openssl libssl0.9.8 build-essential libtool wget http://nginx.org/download/nginx-$NGINX_VERSION.tar.gztar -zxvf nginx-$NGINX_VERSION.tar.gzcd nginx-$NGINX_VERSION./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module --with-pcremake &amp;&amp; make install","categories":[{"name":"Web开发","slug":"Web开发","permalink":"https://flyingglass.github.io/categories/Web开发/"}],"tags":[]},{"title":"mysql配置","slug":"mysql","date":"2018-01-13T12:48:17.000Z","updated":"2021-05-06T09:45:39.724Z","comments":true,"path":"2018/01/13/mysql/","link":"","permalink":"https://flyingglass.github.io/2018/01/13/mysql/","excerpt":"","text":"centos 6.5123yum -y update &amp;&amp; yum -y upgradeyum -y install mysql-serverservice mysqld restart centos 7.01234wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpmrpm -ivh mysql-community-release-el7-5.noarch.rpmyum -y install mysql-community-serverservice mysqld restart Ubuntu 16.04123456789# Removesudo apt-get remove mysql-*dpkg -l |grep ^rc|awk '&#123;print $2&#125;' |sudo xargs dpkg -P# Installsudo apt-get updatesudo apt-get install mysql-servermysql_secure_installationsystemctl status mysql.servicemysqladmin -p -u root version /etc/my.cnf配置1234567891011121314151617181920[root@namenode01 ~]# grep -v &quot;#&quot; /etc/my.cnf | grep -v &quot;^$&quot;[mysqld]datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.socksymbolic-links=0sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLEScharacter-set-server=utf8mb4collation-server=utf8mb4_general_ciinit_connect=&apos;SET NAMES utf8mb4&apos;skip-character-set-client-handshake=true[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid[mysql]default-character-set=utf8mb4[mysqld]lower_case_table_names=1[client]port = 3306default-character-set=utf8mb4 设置用户12grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;root&apos;;flush privileges;","categories":[{"name":"安装配置","slug":"安装配置","permalink":"https://flyingglass.github.io/categories/安装配置/"}],"tags":[]}]}